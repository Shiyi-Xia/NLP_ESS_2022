{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial Three (R): Introduction to Word Embeddings",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shiyi-Xia/NLP_ESS_2022/blob/main/Tutorial_Three_(R)_Introduction_to_Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy2ejTX933iI"
      },
      "source": [
        "# Introduction to Word Embeddings\n",
        "\n",
        "## Douglas Rice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls794rAM39Pb"
      },
      "source": [
        "In this notebook, we'll estimate our first word embedding model, then go through a series of analyses of the estimated embeddings. After completing this notebook, you should be familar with:\n",
        "\n",
        "\n",
        "1. Preparing a corpus for estimating word embeddings\n",
        "2. Estimating a (static) word embedding model\n",
        "3. Analyzing output of (static) word embedding model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBIvCScZBn_g"
      },
      "source": [
        "# GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM5RD3CF7G77"
      },
      "source": [
        "We'll be using the [`text2vec`](http://text2vec.org/index.html) package. `text2vec` was one of the first implementations of  word embeddings functionality in R, and is designed to run *fast*, relatively speaking. Still, it's important to remember that our computational complexity is amping up here, so don't expect immediate results. \n",
        "\n",
        "`text2vec` implements the \"Global Vectors\" (or GloVe) approach for estimating embeddings. Stanford University's [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/) is an approach to estimating a distributional representation of a word. GloVe is based, essentially, on factorizing a huge term co-occurrence matrix. \n",
        "\n",
        "The distributional representation of words means that each term is represented as a distribution over some number of dimensions (say, 3 dimensions, where the values are 0.6, 0.3, and 0.1). This stands in stark contrast to the work we've done to this point, which has effectively encoded each word as being effectively just present (1) or not (0). \n",
        "\n",
        "Perhaps unsurprisingly, the distributional representation better captures semantic meaning than the one-hot encoding. This opens up a world of possibilities for us as researchers. Indeed, this has been a major leap forward for research in Text-as-Data. \n",
        "\n",
        "As an example, we can see how similar one word is to other words by measuring the distance between their distributions. Even more interestingly, we can capture really specific phenomena from text with some simple arithmetic based on word distributions. Consider the following canonical example:\n",
        "\n",
        "- <h2> king - man + woman = queen </h2>\n",
        "\n",
        "Ponder this equation for a moment. From the vector representation of  **king**, we subtract the vector representation of **man**. Then, we add the vector representation of **woman**. The end result of that should be a vector that is very similar to the vector representation of  **queen**. \n",
        "\n",
        "In what follows, we'll work through some examples to see how well this works. I want to caution, though, that the models we are training here are probably too small for us to have too much confidence in the trained models. Nevertheless, you'll see that even with this small set we'll recover really interesting dynamics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka390bNZWhQJ"
      },
      "source": [
        "## Front-end Matters\n",
        "\n",
        "First, let's install the `text2vec` package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OerFHJNPWnHF",
        "outputId": "522441b9-e6f7-4bdd-f2b0-6c59f2891be8"
      },
      "source": [
        "# Installs text2vec package (might take a while)\n",
        "install.packages('text2vec')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘MatrixExtra’, ‘float’, ‘RhpcBLASctl’, ‘RcppArmadillo’, ‘Rcpp’, ‘rsparse’, ‘mlapi’, ‘lgr’\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AARQ4BeMWyNi"
      },
      "source": [
        "And load the library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C00d-gpUWz7u"
      },
      "source": [
        "library(text2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS8Kb23T9Bmu"
      },
      "source": [
        "## PoKi Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfrjO_nHBySr"
      },
      "source": [
        "We'll be using [PoKi](https://github.com/whipson/PoKi-Poems-by-Kids), a corpus of poems written by children and teenagers from grades 1 to 12.\n",
        "\n",
        "One thing to flag right off the bat is the really interesting dynamics related to *who* is writing these posts. We need to keep in mind that the children writing these texts are going to use less formal writing and more imaginative stories. Given this, we'll focus on analogies that are more appropriate for this context; here, we'll aim to create word embeddings that can recreate these two equations:\n",
        "\n",
        "- <h2> cat - meow + bark = dog </h2>\n",
        "\n",
        "- <h2> mom - girl + boy = dad </h2>\n",
        "\n",
        "By the end, we should hopefully be able to recreate these by creating and fitting our GloVe models. But first, let's perform the necessary pre-processing steps before creating our embedding models. \n",
        "\n",
        "Let's download and read in the data:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT7BDwBwCJ5p"
      },
      "source": [
        "# Creates file\n",
        "temp <- tempfile()\n",
        "\n",
        "# Downloads and unzips file to a text8_file variable if it does not exist\n",
        "download.file(\"https://raw.githubusercontent.com/whipson/PoKi-Poems-by-Kids/master/poki.csv\", temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "BJ_6aeU0DVwl",
        "outputId": "6986a2ad-dcbc-4bc8-eff2-3733e9bc5ba0"
      },
      "source": [
        "# Reads in downloaded file\n",
        "poem <- read.csv(temp)\n",
        "\n",
        "# First ten rows\n",
        "head(poem, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 10 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>id</th><th scope=col>title</th><th scope=col>author</th><th scope=col>grade</th><th scope=col>text</th><th scope=col>char</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>104987</td><td>I Love The Zoo                  </td><td>            </td><td>1</td><td>roses are red,  violets are blue.   i love the zoo.   do you?                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td><td> 62</td></tr>\n",
              "\t<tr><th scope=row>2</th><td> 67185</td><td>The scary forest.               </td><td>            </td><td>1</td><td>the forest is really haunted.  i believe it to be so.  but then we are going camping.                                                                                                                                                                                                                                                                                                                                                                                                                                        </td><td> 87</td></tr>\n",
              "\t<tr><th scope=row>3</th><td>103555</td><td>A Hike At School                </td><td>1st grade-wh</td><td>1</td><td>i took a hike at school today  and this is what i saw     bouncing balls      girls chatting against the walls     kids climbing on monkey bars     i even saw some teachers' cars     the wind was blowing my hair in my face     i saw a mud puddle,  but just a trace all of these things i noticed just now on my little hike.                                                                                                                                                                                           </td><td>324</td></tr>\n",
              "\t<tr><th scope=row>4</th><td>112483</td><td>Computer                        </td><td>a           </td><td>1</td><td>you  can  do  what  you  want  you  can play a  game    you can do many things,   you can read and write                                                                                                                                                                                                                                                                                                                                                                                                                     </td><td>106</td></tr>\n",
              "\t<tr><th scope=row>5</th><td> 74516</td><td>Angel                           </td><td>aab         </td><td>1</td><td>angel oh angle you spin like a top angel oh angel you will never stop can't you feel the air  as it blows through your hair angel oh angel itisto bad your a mop!                                                                                                                                                                                                                                                                                                                                                            </td><td>164</td></tr>\n",
              "\t<tr><th scope=row>6</th><td>114693</td><td>Nature Nature and Nature        </td><td>aadhya      </td><td>1</td><td>look at the sun, what a beautiful day.  under the trees, we can run and play.  beauty of nature, we love to see,  from tiny insect to exotic tree.  it is a place to sit and think,  nature and human share the deepest link.  nature has ocean, which is in motion.  nature has tree, nature has river.  if we destroy the nature we would never be free.  our nature keeps us alive,  we must protect it, for society to thrive.  we spoil the nature, we spoil the future.  go along with nature, for your better future. </td><td>491</td></tr>\n",
              "\t<tr><th scope=row>7</th><td> 46453</td><td>Jack                            </td><td>aaliyah     </td><td>1</td><td>dog  playful,  energetic running,  jumping,  tackling my is my friend jack                                                                                                                                                                                                                                                                                                                                                                                                                                                   </td><td> 74</td></tr>\n",
              "\t<tr><th scope=row>8</th><td> 57397</td><td>When I awoke one morning        </td><td>aanna       </td><td>1</td><td>when i awoke one morning,  a dog was on my  head.  i asked ,  ''what are you doing there?' it looked at me and said  ''woof!'' ''wouldn't you like to be outside playing?''said the man ''i'm staying here and playing here. '' said the dog he played all night and day. he came inside his new house and played inside a wet wet day.                                                                                                                                                                                      </td><td>325</td></tr>\n",
              "\t<tr><th scope=row>9</th><td> 77201</td><td>My Blue Berries and  My Cherries</td><td>aarathi     </td><td>1</td><td>i went to my blue berry tree they were no blue berries found i went to another tree to get some more free but found none but cherries round.                                                                                                                                                                                                                                                                                                                                                                                 </td><td>143</td></tr>\n",
              "\t<tr><th scope=row>10</th><td> 40520</td><td>A snowy day                     </td><td>ab.         </td><td>1</td><td>one snowy day the children went outside to play in the snow.  they threw snowballs,  went sledding and made a snowman.  afterwards they went inside to drink warm hot chocolate.  it was a fun snowy day                                                                                                                                                                                                                                                                                                                     </td><td>199</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 10 × 6\n\n| <!--/--> | id &lt;int&gt; | title &lt;chr&gt; | author &lt;chr&gt; | grade &lt;int&gt; | text &lt;chr&gt; | char &lt;int&gt; |\n|---|---|---|---|---|---|---|\n| 1 | 104987 | I Love The Zoo                   | <!----> | 1 | roses are red,  violets are blue.   i love the zoo.   do you?                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |  62 |\n| 2 |  67185 | The scary forest.                | <!----> | 1 | the forest is really haunted.  i believe it to be so.  but then we are going camping.                                                                                                                                                                                                                                                                                                                                                                                                                                         |  87 |\n| 3 | 103555 | A Hike At School                 | 1st grade-wh | 1 | i took a hike at school today  and this is what i saw     bouncing balls      girls chatting against the walls     kids climbing on monkey bars     i even saw some teachers' cars     the wind was blowing my hair in my face     i saw a mud puddle,  but just a trace all of these things i noticed just now on my little hike.                                                                                                                                                                                            | 324 |\n| 4 | 112483 | Computer                         | a            | 1 | you  can  do  what  you  want  you  can play a  game    you can do many things,   you can read and write                                                                                                                                                                                                                                                                                                                                                                                                                      | 106 |\n| 5 |  74516 | Angel                            | aab          | 1 | angel oh angle you spin like a top angel oh angel you will never stop can't you feel the air  as it blows through your hair angel oh angel itisto bad your a mop!                                                                                                                                                                                                                                                                                                                                                             | 164 |\n| 6 | 114693 | Nature Nature and Nature         | aadhya       | 1 | look at the sun, what a beautiful day.  under the trees, we can run and play.  beauty of nature, we love to see,  from tiny insect to exotic tree.  it is a place to sit and think,  nature and human share the deepest link.  nature has ocean, which is in motion.  nature has tree, nature has river.  if we destroy the nature we would never be free.  our nature keeps us alive,  we must protect it, for society to thrive.  we spoil the nature, we spoil the future.  go along with nature, for your better future.  | 491 |\n| 7 |  46453 | Jack                             | aaliyah      | 1 | dog  playful,  energetic running,  jumping,  tackling my is my friend jack                                                                                                                                                                                                                                                                                                                                                                                                                                                    |  74 |\n| 8 |  57397 | When I awoke one morning         | aanna        | 1 | when i awoke one morning,  a dog was on my  head.  i asked ,  ''what are you doing there?' it looked at me and said  ''woof!'' ''wouldn't you like to be outside playing?''said the man ''i'm staying here and playing here. '' said the dog he played all night and day. he came inside his new house and played inside a wet wet day.                                                                                                                                                                                       | 325 |\n| 9 |  77201 | My Blue Berries and  My Cherries | aarathi      | 1 | i went to my blue berry tree they were no blue berries found i went to another tree to get some more free but found none but cherries round.                                                                                                                                                                                                                                                                                                                                                                                  | 143 |\n| 10 |  40520 | A snowy day                      | ab.          | 1 | one snowy day the children went outside to play in the snow.  they threw snowballs,  went sledding and made a snowman.  afterwards they went inside to drink warm hot chocolate.  it was a fun snowy day                                                                                                                                                                                                                                                                                                                      | 199 |\n\n",
            "text/latex": "A data.frame: 10 × 6\n\\begin{tabular}{r|llllll}\n  & id & title & author & grade & text & char\\\\\n  & <int> & <chr> & <chr> & <int> & <chr> & <int>\\\\\n\\hline\n\t1 & 104987 & I Love The Zoo                   &              & 1 & roses are red,  violets are blue.   i love the zoo.   do you?                                                                                                                                                                                                                                                                                                                                                                                                                                                                 &  62\\\\\n\t2 &  67185 & The scary forest.                &              & 1 & the forest is really haunted.  i believe it to be so.  but then we are going camping.                                                                                                                                                                                                                                                                                                                                                                                                                                         &  87\\\\\n\t3 & 103555 & A Hike At School                 & 1st grade-wh & 1 & i took a hike at school today  and this is what i saw     bouncing balls      girls chatting against the walls     kids climbing on monkey bars     i even saw some teachers' cars     the wind was blowing my hair in my face     i saw a mud puddle,  but just a trace all of these things i noticed just now on my little hike.                                                                                                                                                                                            & 324\\\\\n\t4 & 112483 & Computer                         & a            & 1 & you  can  do  what  you  want  you  can play a  game    you can do many things,   you can read and write                                                                                                                                                                                                                                                                                                                                                                                                                      & 106\\\\\n\t5 &  74516 & Angel                            & aab          & 1 & angel oh angle you spin like a top angel oh angel you will never stop can't you feel the air  as it blows through your hair angel oh angel itisto bad your a mop!                                                                                                                                                                                                                                                                                                                                                             & 164\\\\\n\t6 & 114693 & Nature Nature and Nature         & aadhya       & 1 & look at the sun, what a beautiful day.  under the trees, we can run and play.  beauty of nature, we love to see,  from tiny insect to exotic tree.  it is a place to sit and think,  nature and human share the deepest link.  nature has ocean, which is in motion.  nature has tree, nature has river.  if we destroy the nature we would never be free.  our nature keeps us alive,  we must protect it, for society to thrive.  we spoil the nature, we spoil the future.  go along with nature, for your better future.  & 491\\\\\n\t7 &  46453 & Jack                             & aaliyah      & 1 & dog  playful,  energetic running,  jumping,  tackling my is my friend jack                                                                                                                                                                                                                                                                                                                                                                                                                                                    &  74\\\\\n\t8 &  57397 & When I awoke one morning         & aanna        & 1 & when i awoke one morning,  a dog was on my  head.  i asked ,  ''what are you doing there?' it looked at me and said  ''woof!'' ''wouldn't you like to be outside playing?''said the man ''i'm staying here and playing here. '' said the dog he played all night and day. he came inside his new house and played inside a wet wet day.                                                                                                                                                                                       & 325\\\\\n\t9 &  77201 & My Blue Berries and  My Cherries & aarathi      & 1 & i went to my blue berry tree they were no blue berries found i went to another tree to get some more free but found none but cherries round.                                                                                                                                                                                                                                                                                                                                                                                  & 143\\\\\n\t10 &  40520 & A snowy day                      & ab.          & 1 & one snowy day the children went outside to play in the snow.  they threw snowballs,  went sledding and made a snowman.  afterwards they went inside to drink warm hot chocolate.  it was a fun snowy day                                                                                                                                                                                                                                                                                                                      & 199\\\\\n\\end{tabular}\n",
            "text/plain": [
              "   id     title                            author       grade\n",
              "1  104987 I Love The Zoo                                1    \n",
              "2   67185 The scary forest.                             1    \n",
              "3  103555 A Hike At School                 1st grade-wh 1    \n",
              "4  112483 Computer                         a            1    \n",
              "5   74516 Angel                            aab          1    \n",
              "6  114693 Nature Nature and Nature         aadhya       1    \n",
              "7   46453 Jack                             aaliyah      1    \n",
              "8   57397 When I awoke one morning         aanna        1    \n",
              "9   77201 My Blue Berries and  My Cherries aarathi      1    \n",
              "10  40520 A snowy day                      ab.          1    \n",
              "   text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
              "1  roses are red,  violets are blue.   i love the zoo.   do you?                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
              "2  the forest is really haunted.  i believe it to be so.  but then we are going camping.                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
              "3  i took a hike at school today  and this is what i saw     bouncing balls      girls chatting against the walls     kids climbing on monkey bars     i even saw some teachers' cars     the wind was blowing my hair in my face     i saw a mud puddle,  but just a trace all of these things i noticed just now on my little hike.                                                                                                                                                                                           \n",
              "4  you  can  do  what  you  want  you  can play a  game    you can do many things,   you can read and write                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
              "5  angel oh angle you spin like a top angel oh angel you will never stop can't you feel the air  as it blows through your hair angel oh angel itisto bad your a mop!                                                                                                                                                                                                                                                                                                                                                            \n",
              "6  look at the sun, what a beautiful day.  under the trees, we can run and play.  beauty of nature, we love to see,  from tiny insect to exotic tree.  it is a place to sit and think,  nature and human share the deepest link.  nature has ocean, which is in motion.  nature has tree, nature has river.  if we destroy the nature we would never be free.  our nature keeps us alive,  we must protect it, for society to thrive.  we spoil the nature, we spoil the future.  go along with nature, for your better future. \n",
              "7  dog  playful,  energetic running,  jumping,  tackling my is my friend jack                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
              "8  when i awoke one morning,  a dog was on my  head.  i asked ,  ''what are you doing there?' it looked at me and said  ''woof!'' ''wouldn't you like to be outside playing?''said the man ''i'm staying here and playing here. '' said the dog he played all night and day. he came inside his new house and played inside a wet wet day.                                                                                                                                                                                      \n",
              "9  i went to my blue berry tree they were no blue berries found i went to another tree to get some more free but found none but cherries round.                                                                                                                                                                                                                                                                                                                                                                                 \n",
              "10 one snowy day the children went outside to play in the snow.  they threw snowballs,  went sledding and made a snowman.  afterwards they went inside to drink warm hot chocolate.  it was a fun snowy day                                                                                                                                                                                                                                                                                                                     \n",
              "   char\n",
              "1   62 \n",
              "2   87 \n",
              "3  324 \n",
              "4  106 \n",
              "5  164 \n",
              "6  491 \n",
              "7   74 \n",
              "8  325 \n",
              "9  143 \n",
              "10 199 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "HPpZMH5kXnsd",
        "outputId": "5762c124-c6f2-4ac5-86d8-9f42366a9765"
      },
      "source": [
        "# Checks dimensions\n",
        "dim(poem)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>61508</li><li>6</li></ol>\n"
            ],
            "text/markdown": "1. 61508\n2. 6\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 61508\n\\item 6\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 61508     6"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_flemCsX4L7"
      },
      "source": [
        "We want the poems themselves, so we'll use the column 'text' for tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMhYOwTu866T"
      },
      "source": [
        "## Tokenization and Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfqH5NO2I7CB"
      },
      "source": [
        "We start with `text2vec` by creating a tokenized iterator and vectorized vocabulary first. This time, there's no need to lowercase our words since the downloaded dataset is already lowercased.\n",
        "\n",
        "Let's tokenize the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "HgpMIqt2JPxL",
        "outputId": "265baf5a-d137-4a06-a5db-16392c655522"
      },
      "source": [
        "# Tokenization\n",
        "tokens <- word_tokenizer(poem$text)\n",
        "\n",
        "# First five rows tokenized\n",
        "head(tokens, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'roses'</li><li>'are'</li><li>'red'</li><li>'violets'</li><li>'are'</li><li>'blue'</li><li>'i'</li><li>'love'</li><li>'the'</li><li>'zoo'</li><li>'do'</li><li>'you'</li></ol>\n",
              "</li>\n",
              "\t<li><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'the'</li><li>'forest'</li><li>'is'</li><li>'really'</li><li>'haunted'</li><li>'i'</li><li>'believe'</li><li>'it'</li><li>'to'</li><li>'be'</li><li>'so'</li><li>'but'</li><li>'then'</li><li>'we'</li><li>'are'</li><li>'going'</li><li>'camping'</li></ol>\n",
              "</li>\n",
              "\t<li><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'i'</li><li>'took'</li><li>'a'</li><li>'hike'</li><li>'at'</li><li>'school'</li><li>'today'</li><li>'and'</li><li>'this'</li><li>'is'</li><li>'what'</li><li>'i'</li><li>'saw'</li><li>'bouncing'</li><li>'balls'</li><li>'girls'</li><li>'chatting'</li><li>'against'</li><li>'the'</li><li>'walls'</li><li>'kids'</li><li>'climbing'</li><li>'on'</li><li>'monkey'</li><li>'bars'</li><li>'i'</li><li>'even'</li><li>'saw'</li><li>'some'</li><li>'teachers'</li><li>'cars'</li><li>'the'</li><li>'wind'</li><li>'was'</li><li>'blowing'</li><li>'my'</li><li>'hair'</li><li>'in'</li><li>'my'</li><li>'face'</li><li>'i'</li><li>'saw'</li><li>'a'</li><li>'mud'</li><li>'puddle'</li><li>'but'</li><li>'just'</li><li>'a'</li><li>'trace'</li><li>'all'</li><li>'of'</li><li>'these'</li><li>'things'</li><li>'i'</li><li>'noticed'</li><li>'just'</li><li>'now'</li><li>'on'</li><li>'my'</li><li>'little'</li><li>'hike'</li></ol>\n",
              "</li>\n",
              "\t<li><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'you'</li><li>'can'</li><li>'do'</li><li>'what'</li><li>'you'</li><li>'want'</li><li>'you'</li><li>'can'</li><li>'play'</li><li>'a'</li><li>'game'</li><li>'you'</li><li>'can'</li><li>'do'</li><li>'many'</li><li>'things'</li><li>'you'</li><li>'can'</li><li>'read'</li><li>'and'</li><li>'write'</li></ol>\n",
              "</li>\n",
              "\t<li><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'angel'</li><li>'oh'</li><li>'angle'</li><li>'you'</li><li>'spin'</li><li>'like'</li><li>'a'</li><li>'top'</li><li>'angel'</li><li>'oh'</li><li>'angel'</li><li>'you'</li><li>'will'</li><li>'never'</li><li>'stop'</li><li>'can\\'t'</li><li>'you'</li><li>'feel'</li><li>'the'</li><li>'air'</li><li>'as'</li><li>'it'</li><li>'blows'</li><li>'through'</li><li>'your'</li><li>'hair'</li><li>'angel'</li><li>'oh'</li><li>'angel'</li><li>'itisto'</li><li>'bad'</li><li>'your'</li><li>'a'</li><li>'mop'</li></ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/markdown": "1. 1. 'roses'\n2. 'are'\n3. 'red'\n4. 'violets'\n5. 'are'\n6. 'blue'\n7. 'i'\n8. 'love'\n9. 'the'\n10. 'zoo'\n11. 'do'\n12. 'you'\n\n\n\n2. 1. 'the'\n2. 'forest'\n3. 'is'\n4. 'really'\n5. 'haunted'\n6. 'i'\n7. 'believe'\n8. 'it'\n9. 'to'\n10. 'be'\n11. 'so'\n12. 'but'\n13. 'then'\n14. 'we'\n15. 'are'\n16. 'going'\n17. 'camping'\n\n\n\n3. 1. 'i'\n2. 'took'\n3. 'a'\n4. 'hike'\n5. 'at'\n6. 'school'\n7. 'today'\n8. 'and'\n9. 'this'\n10. 'is'\n11. 'what'\n12. 'i'\n13. 'saw'\n14. 'bouncing'\n15. 'balls'\n16. 'girls'\n17. 'chatting'\n18. 'against'\n19. 'the'\n20. 'walls'\n21. 'kids'\n22. 'climbing'\n23. 'on'\n24. 'monkey'\n25. 'bars'\n26. 'i'\n27. 'even'\n28. 'saw'\n29. 'some'\n30. 'teachers'\n31. 'cars'\n32. 'the'\n33. 'wind'\n34. 'was'\n35. 'blowing'\n36. 'my'\n37. 'hair'\n38. 'in'\n39. 'my'\n40. 'face'\n41. 'i'\n42. 'saw'\n43. 'a'\n44. 'mud'\n45. 'puddle'\n46. 'but'\n47. 'just'\n48. 'a'\n49. 'trace'\n50. 'all'\n51. 'of'\n52. 'these'\n53. 'things'\n54. 'i'\n55. 'noticed'\n56. 'just'\n57. 'now'\n58. 'on'\n59. 'my'\n60. 'little'\n61. 'hike'\n\n\n\n4. 1. 'you'\n2. 'can'\n3. 'do'\n4. 'what'\n5. 'you'\n6. 'want'\n7. 'you'\n8. 'can'\n9. 'play'\n10. 'a'\n11. 'game'\n12. 'you'\n13. 'can'\n14. 'do'\n15. 'many'\n16. 'things'\n17. 'you'\n18. 'can'\n19. 'read'\n20. 'and'\n21. 'write'\n\n\n\n5. 1. 'angel'\n2. 'oh'\n3. 'angle'\n4. 'you'\n5. 'spin'\n6. 'like'\n7. 'a'\n8. 'top'\n9. 'angel'\n10. 'oh'\n11. 'angel'\n12. 'you'\n13. 'will'\n14. 'never'\n15. 'stop'\n16. 'can\\'t'\n17. 'you'\n18. 'feel'\n19. 'the'\n20. 'air'\n21. 'as'\n22. 'it'\n23. 'blows'\n24. 'through'\n25. 'your'\n26. 'hair'\n27. 'angel'\n28. 'oh'\n29. 'angel'\n30. 'itisto'\n31. 'bad'\n32. 'your'\n33. 'a'\n34. 'mop'\n\n\n\n\n\n",
            "text/latex": "\\begin{enumerate}\n\\item \\begin{enumerate*}\n\\item 'roses'\n\\item 'are'\n\\item 'red'\n\\item 'violets'\n\\item 'are'\n\\item 'blue'\n\\item 'i'\n\\item 'love'\n\\item 'the'\n\\item 'zoo'\n\\item 'do'\n\\item 'you'\n\\end{enumerate*}\n\n\\item \\begin{enumerate*}\n\\item 'the'\n\\item 'forest'\n\\item 'is'\n\\item 'really'\n\\item 'haunted'\n\\item 'i'\n\\item 'believe'\n\\item 'it'\n\\item 'to'\n\\item 'be'\n\\item 'so'\n\\item 'but'\n\\item 'then'\n\\item 'we'\n\\item 'are'\n\\item 'going'\n\\item 'camping'\n\\end{enumerate*}\n\n\\item \\begin{enumerate*}\n\\item 'i'\n\\item 'took'\n\\item 'a'\n\\item 'hike'\n\\item 'at'\n\\item 'school'\n\\item 'today'\n\\item 'and'\n\\item 'this'\n\\item 'is'\n\\item 'what'\n\\item 'i'\n\\item 'saw'\n\\item 'bouncing'\n\\item 'balls'\n\\item 'girls'\n\\item 'chatting'\n\\item 'against'\n\\item 'the'\n\\item 'walls'\n\\item 'kids'\n\\item 'climbing'\n\\item 'on'\n\\item 'monkey'\n\\item 'bars'\n\\item 'i'\n\\item 'even'\n\\item 'saw'\n\\item 'some'\n\\item 'teachers'\n\\item 'cars'\n\\item 'the'\n\\item 'wind'\n\\item 'was'\n\\item 'blowing'\n\\item 'my'\n\\item 'hair'\n\\item 'in'\n\\item 'my'\n\\item 'face'\n\\item 'i'\n\\item 'saw'\n\\item 'a'\n\\item 'mud'\n\\item 'puddle'\n\\item 'but'\n\\item 'just'\n\\item 'a'\n\\item 'trace'\n\\item 'all'\n\\item 'of'\n\\item 'these'\n\\item 'things'\n\\item 'i'\n\\item 'noticed'\n\\item 'just'\n\\item 'now'\n\\item 'on'\n\\item 'my'\n\\item 'little'\n\\item 'hike'\n\\end{enumerate*}\n\n\\item \\begin{enumerate*}\n\\item 'you'\n\\item 'can'\n\\item 'do'\n\\item 'what'\n\\item 'you'\n\\item 'want'\n\\item 'you'\n\\item 'can'\n\\item 'play'\n\\item 'a'\n\\item 'game'\n\\item 'you'\n\\item 'can'\n\\item 'do'\n\\item 'many'\n\\item 'things'\n\\item 'you'\n\\item 'can'\n\\item 'read'\n\\item 'and'\n\\item 'write'\n\\end{enumerate*}\n\n\\item \\begin{enumerate*}\n\\item 'angel'\n\\item 'oh'\n\\item 'angle'\n\\item 'you'\n\\item 'spin'\n\\item 'like'\n\\item 'a'\n\\item 'top'\n\\item 'angel'\n\\item 'oh'\n\\item 'angel'\n\\item 'you'\n\\item 'will'\n\\item 'never'\n\\item 'stop'\n\\item 'can\\textbackslash{}'t'\n\\item 'you'\n\\item 'feel'\n\\item 'the'\n\\item 'air'\n\\item 'as'\n\\item 'it'\n\\item 'blows'\n\\item 'through'\n\\item 'your'\n\\item 'hair'\n\\item 'angel'\n\\item 'oh'\n\\item 'angel'\n\\item 'itisto'\n\\item 'bad'\n\\item 'your'\n\\item 'a'\n\\item 'mop'\n\\end{enumerate*}\n\n\\end{enumerate}\n",
            "text/plain": [
              "[[1]]\n",
              " [1] \"roses\"   \"are\"     \"red\"     \"violets\" \"are\"     \"blue\"    \"i\"      \n",
              " [8] \"love\"    \"the\"     \"zoo\"     \"do\"      \"you\"    \n",
              "\n",
              "[[2]]\n",
              " [1] \"the\"     \"forest\"  \"is\"      \"really\"  \"haunted\" \"i\"       \"believe\"\n",
              " [8] \"it\"      \"to\"      \"be\"      \"so\"      \"but\"     \"then\"    \"we\"     \n",
              "[15] \"are\"     \"going\"   \"camping\"\n",
              "\n",
              "[[3]]\n",
              " [1] \"i\"        \"took\"     \"a\"        \"hike\"     \"at\"       \"school\"  \n",
              " [7] \"today\"    \"and\"      \"this\"     \"is\"       \"what\"     \"i\"       \n",
              "[13] \"saw\"      \"bouncing\" \"balls\"    \"girls\"    \"chatting\" \"against\" \n",
              "[19] \"the\"      \"walls\"    \"kids\"     \"climbing\" \"on\"       \"monkey\"  \n",
              "[25] \"bars\"     \"i\"        \"even\"     \"saw\"      \"some\"     \"teachers\"\n",
              "[31] \"cars\"     \"the\"      \"wind\"     \"was\"      \"blowing\"  \"my\"      \n",
              "[37] \"hair\"     \"in\"       \"my\"       \"face\"     \"i\"        \"saw\"     \n",
              "[43] \"a\"        \"mud\"      \"puddle\"   \"but\"      \"just\"     \"a\"       \n",
              "[49] \"trace\"    \"all\"      \"of\"       \"these\"    \"things\"   \"i\"       \n",
              "[55] \"noticed\"  \"just\"     \"now\"      \"on\"       \"my\"       \"little\"  \n",
              "[61] \"hike\"    \n",
              "\n",
              "[[4]]\n",
              " [1] \"you\"    \"can\"    \"do\"     \"what\"   \"you\"    \"want\"   \"you\"    \"can\"   \n",
              " [9] \"play\"   \"a\"      \"game\"   \"you\"    \"can\"    \"do\"     \"many\"   \"things\"\n",
              "[17] \"you\"    \"can\"    \"read\"   \"and\"    \"write\" \n",
              "\n",
              "[[5]]\n",
              " [1] \"angel\"   \"oh\"      \"angle\"   \"you\"     \"spin\"    \"like\"    \"a\"      \n",
              " [8] \"top\"     \"angel\"   \"oh\"      \"angel\"   \"you\"     \"will\"    \"never\"  \n",
              "[15] \"stop\"    \"can't\"   \"you\"     \"feel\"    \"the\"     \"air\"     \"as\"     \n",
              "[22] \"it\"      \"blows\"   \"through\" \"your\"    \"hair\"    \"angel\"   \"oh\"     \n",
              "[29] \"angel\"   \"itisto\"  \"bad\"     \"your\"    \"a\"       \"mop\"    \n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-hQjA1-YXsO"
      },
      "source": [
        "Create an iterator object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKLUOG6xYZo1"
      },
      "source": [
        "# Create iterator object \n",
        "it <- itoken(tokens, progressbar = FALSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw_phDKOY7P5"
      },
      "source": [
        "Build the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hyIXeVobZGz4",
        "outputId": "c3cf7740-3630-4c5b-89f4-1a3c242aeb7e"
      },
      "source": [
        "# Build vocabulary\n",
        "vocab <- create_vocabulary(it)\n",
        "\n",
        "# Vocabulary\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A text2vec_vocabulary: 56474 × 3</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>term</th><th scope=col>term_count</th><th scope=col>doc_count</th></tr>\n",
              "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td>0000     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0000000  </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0000001  </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>00a:m    </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>00he     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>00o'clock</td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>00p      </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>02       </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>04       </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>05at     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>05â’n    </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>080      </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0f       </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0ften    </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0min     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0nce     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0ne      </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0sec     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0â’colck </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>1'000'000</td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>10000    </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>100000   </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>1000000  </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>100k     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>100miles </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>100s     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>100th    </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>103      </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>104      </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>10foot2  </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
              "\t<tr><td>your</td><td> 14900</td><td> 8083</td></tr>\n",
              "\t<tr><td>she </td><td> 14949</td><td> 5126</td></tr>\n",
              "\t<tr><td>love</td><td> 15086</td><td> 8373</td></tr>\n",
              "\t<tr><td>they</td><td> 16807</td><td> 8281</td></tr>\n",
              "\t<tr><td>all </td><td> 17526</td><td>12199</td></tr>\n",
              "\t<tr><td>as  </td><td> 17526</td><td> 8394</td></tr>\n",
              "\t<tr><td>for </td><td> 17641</td><td>11241</td></tr>\n",
              "\t<tr><td>be  </td><td> 18190</td><td>11224</td></tr>\n",
              "\t<tr><td>when</td><td> 19313</td><td>12871</td></tr>\n",
              "\t<tr><td>with</td><td> 19938</td><td>13445</td></tr>\n",
              "\t<tr><td>so  </td><td> 21107</td><td>13870</td></tr>\n",
              "\t<tr><td>on  </td><td> 21518</td><td>14943</td></tr>\n",
              "\t<tr><td>was </td><td> 21549</td><td>10842</td></tr>\n",
              "\t<tr><td>but </td><td> 22095</td><td>14606</td></tr>\n",
              "\t<tr><td>he  </td><td> 23323</td><td> 8521</td></tr>\n",
              "\t<tr><td>like</td><td> 24288</td><td>14832</td></tr>\n",
              "\t<tr><td>that</td><td> 24999</td><td>14575</td></tr>\n",
              "\t<tr><td>are </td><td> 26681</td><td>14887</td></tr>\n",
              "\t<tr><td>me  </td><td> 29851</td><td>15628</td></tr>\n",
              "\t<tr><td>of  </td><td> 34025</td><td>18832</td></tr>\n",
              "\t<tr><td>in  </td><td> 37990</td><td>22586</td></tr>\n",
              "\t<tr><td>it  </td><td> 39444</td><td>19604</td></tr>\n",
              "\t<tr><td>you </td><td> 58337</td><td>19627</td></tr>\n",
              "\t<tr><td>my  </td><td> 58604</td><td>24914</td></tr>\n",
              "\t<tr><td>is  </td><td> 58608</td><td>27119</td></tr>\n",
              "\t<tr><td>to  </td><td> 69175</td><td>30347</td></tr>\n",
              "\t<tr><td>and </td><td> 80863</td><td>34798</td></tr>\n",
              "\t<tr><td>a   </td><td> 92765</td><td>37607</td></tr>\n",
              "\t<tr><td>the </td><td>120677</td><td>37676</td></tr>\n",
              "\t<tr><td>i   </td><td>124832</td><td>32777</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA text2vec_vocabulary: 56474 × 3\n\n| term &lt;chr&gt; | term_count &lt;int&gt; | doc_count &lt;int&gt; |\n|---|---|---|\n| 0000      | 1 | 1 |\n| 0000000   | 1 | 1 |\n| 0000001   | 1 | 1 |\n| 00a:m     | 1 | 1 |\n| 00he      | 1 | 1 |\n| 00o'clock | 1 | 1 |\n| 00p       | 1 | 1 |\n| 02        | 1 | 1 |\n| 04        | 1 | 1 |\n| 05at      | 1 | 1 |\n| 05â’n     | 1 | 1 |\n| 080       | 1 | 1 |\n| 0f        | 1 | 1 |\n| 0ften     | 1 | 1 |\n| 0min      | 1 | 1 |\n| 0nce      | 1 | 1 |\n| 0ne       | 1 | 1 |\n| 0sec      | 1 | 1 |\n| 0â’colck  | 1 | 1 |\n| 1'000'000 | 1 | 1 |\n| 10000     | 1 | 1 |\n| 100000    | 1 | 1 |\n| 1000000   | 1 | 1 |\n| 100k      | 1 | 1 |\n| 100miles  | 1 | 1 |\n| 100s      | 1 | 1 |\n| 100th     | 1 | 1 |\n| 103       | 1 | 1 |\n| 104       | 1 | 1 |\n| 10foot2   | 1 | 1 |\n| ⋮ | ⋮ | ⋮ |\n| your |  14900 |  8083 |\n| she  |  14949 |  5126 |\n| love |  15086 |  8373 |\n| they |  16807 |  8281 |\n| all  |  17526 | 12199 |\n| as   |  17526 |  8394 |\n| for  |  17641 | 11241 |\n| be   |  18190 | 11224 |\n| when |  19313 | 12871 |\n| with |  19938 | 13445 |\n| so   |  21107 | 13870 |\n| on   |  21518 | 14943 |\n| was  |  21549 | 10842 |\n| but  |  22095 | 14606 |\n| he   |  23323 |  8521 |\n| like |  24288 | 14832 |\n| that |  24999 | 14575 |\n| are  |  26681 | 14887 |\n| me   |  29851 | 15628 |\n| of   |  34025 | 18832 |\n| in   |  37990 | 22586 |\n| it   |  39444 | 19604 |\n| you  |  58337 | 19627 |\n| my   |  58604 | 24914 |\n| is   |  58608 | 27119 |\n| to   |  69175 | 30347 |\n| and  |  80863 | 34798 |\n| a    |  92765 | 37607 |\n| the  | 120677 | 37676 |\n| i    | 124832 | 32777 |\n\n",
            "text/latex": "A text2vec\\_vocabulary: 56474 × 3\n\\begin{tabular}{lll}\n term & term\\_count & doc\\_count\\\\\n <chr> & <int> & <int>\\\\\n\\hline\n\t 0000      & 1 & 1\\\\\n\t 0000000   & 1 & 1\\\\\n\t 0000001   & 1 & 1\\\\\n\t 00a:m     & 1 & 1\\\\\n\t 00he      & 1 & 1\\\\\n\t 00o'clock & 1 & 1\\\\\n\t 00p       & 1 & 1\\\\\n\t 02        & 1 & 1\\\\\n\t 04        & 1 & 1\\\\\n\t 05at      & 1 & 1\\\\\n\t 05â’n     & 1 & 1\\\\\n\t 080       & 1 & 1\\\\\n\t 0f        & 1 & 1\\\\\n\t 0ften     & 1 & 1\\\\\n\t 0min      & 1 & 1\\\\\n\t 0nce      & 1 & 1\\\\\n\t 0ne       & 1 & 1\\\\\n\t 0sec      & 1 & 1\\\\\n\t 0â’colck  & 1 & 1\\\\\n\t 1'000'000 & 1 & 1\\\\\n\t 10000     & 1 & 1\\\\\n\t 100000    & 1 & 1\\\\\n\t 1000000   & 1 & 1\\\\\n\t 100k      & 1 & 1\\\\\n\t 100miles  & 1 & 1\\\\\n\t 100s      & 1 & 1\\\\\n\t 100th     & 1 & 1\\\\\n\t 103       & 1 & 1\\\\\n\t 104       & 1 & 1\\\\\n\t 10foot2   & 1 & 1\\\\\n\t ⋮ & ⋮ & ⋮\\\\\n\t your &  14900 &  8083\\\\\n\t she  &  14949 &  5126\\\\\n\t love &  15086 &  8373\\\\\n\t they &  16807 &  8281\\\\\n\t all  &  17526 & 12199\\\\\n\t as   &  17526 &  8394\\\\\n\t for  &  17641 & 11241\\\\\n\t be   &  18190 & 11224\\\\\n\t when &  19313 & 12871\\\\\n\t with &  19938 & 13445\\\\\n\t so   &  21107 & 13870\\\\\n\t on   &  21518 & 14943\\\\\n\t was  &  21549 & 10842\\\\\n\t but  &  22095 & 14606\\\\\n\t he   &  23323 &  8521\\\\\n\t like &  24288 & 14832\\\\\n\t that &  24999 & 14575\\\\\n\t are  &  26681 & 14887\\\\\n\t me   &  29851 & 15628\\\\\n\t of   &  34025 & 18832\\\\\n\t in   &  37990 & 22586\\\\\n\t it   &  39444 & 19604\\\\\n\t you  &  58337 & 19627\\\\\n\t my   &  58604 & 24914\\\\\n\t is   &  58608 & 27119\\\\\n\t to   &  69175 & 30347\\\\\n\t and  &  80863 & 34798\\\\\n\t a    &  92765 & 37607\\\\\n\t the  & 120677 & 37676\\\\\n\t i    & 124832 & 32777\\\\\n\\end{tabular}\n",
            "text/plain": [
              "      term      term_count doc_count\n",
              "1     0000      1          1        \n",
              "2     0000000   1          1        \n",
              "3     0000001   1          1        \n",
              "4     00a:m     1          1        \n",
              "5     00he      1          1        \n",
              "6     00o'clock 1          1        \n",
              "7     00p       1          1        \n",
              "8     02        1          1        \n",
              "9     04        1          1        \n",
              "10    05at      1          1        \n",
              "11    05â’n     1          1        \n",
              "12    080       1          1        \n",
              "13    0f        1          1        \n",
              "14    0ften     1          1        \n",
              "15    0min      1          1        \n",
              "16    0nce      1          1        \n",
              "17    0ne       1          1        \n",
              "18    0sec      1          1        \n",
              "19    0â’colck  1          1        \n",
              "20    1'000'000 1          1        \n",
              "21    10000     1          1        \n",
              "22    100000    1          1        \n",
              "23    1000000   1          1        \n",
              "24    100k      1          1        \n",
              "25    100miles  1          1        \n",
              "26    100s      1          1        \n",
              "27    100th     1          1        \n",
              "28    103       1          1        \n",
              "29    104       1          1        \n",
              "30    10foot2   1          1        \n",
              "⋮     ⋮         ⋮          ⋮        \n",
              "56445 your       14900      8083    \n",
              "56446 she        14949      5126    \n",
              "56447 love       15086      8373    \n",
              "56448 they       16807      8281    \n",
              "56449 all        17526     12199    \n",
              "56450 as         17526      8394    \n",
              "56451 for        17641     11241    \n",
              "56452 be         18190     11224    \n",
              "56453 when       19313     12871    \n",
              "56454 with       19938     13445    \n",
              "56455 so         21107     13870    \n",
              "56456 on         21518     14943    \n",
              "56457 was        21549     10842    \n",
              "56458 but        22095     14606    \n",
              "56459 he         23323      8521    \n",
              "56460 like       24288     14832    \n",
              "56461 that       24999     14575    \n",
              "56462 are        26681     14887    \n",
              "56463 me         29851     15628    \n",
              "56464 of         34025     18832    \n",
              "56465 in         37990     22586    \n",
              "56466 it         39444     19604    \n",
              "56467 you        58337     19627    \n",
              "56468 my         58604     24914    \n",
              "56469 is         58608     27119    \n",
              "56470 to         69175     30347    \n",
              "56471 and        80863     34798    \n",
              "56472 a          92765     37607    \n",
              "56473 the       120677     37676    \n",
              "56474 i         124832     32777    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "w9fpfk4WZYrM",
        "outputId": "29f2c033-bd91-4515-e969-7b106e83a4aa"
      },
      "source": [
        "# Check dimensions\n",
        "dim(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>56474</li><li>3</li></ol>\n"
            ],
            "text/markdown": "1. 56474\n2. 3\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 56474\n\\item 3\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 56474     3"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qye3rnkZIVT"
      },
      "source": [
        "And prune and vectorize it. We'll keep the terms that occur at least 5 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "hgLAjnr-ZLAR",
        "outputId": "f6784a34-4ecd-4585-ad9b-2bb3d009c14d"
      },
      "source": [
        "# Prune vocabulary\n",
        "vocab <- prune_vocabulary(vocab, term_count_min = 5)\n",
        "\n",
        "# Check dimensions\n",
        "dim(vocab)\n",
        "\n",
        "# Vectorize\n",
        "vectorizer <- vocab_vectorizer(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>14267</li><li>3</li></ol>\n"
            ],
            "text/markdown": "1. 14267\n2. 3\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 14267\n\\item 3\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 14267     3"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMpN2L5IZseV"
      },
      "source": [
        "As we can see, pruning our vocabulary deleted over 40 thousand words. I want to reiterate that this is a *very small* corpus from the perspective of traditional word embedding models. When we are working with word representations trained with these smaller corpora, we should be really cautious in our approach. \n",
        "\n",
        "Moving on, we can create out term-co-occurence matrix (TCM). We can achieve different results by experimenting with the `skip_grams_window` and other parameters. The definition of whether two words occur together is arbitrary, so we definitely want to play around with the parameters to see the different results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI6x2HUXaDUp"
      },
      "source": [
        "# use window of 5 for context words\n",
        "tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugs_AQsF1BEy"
      },
      "source": [
        "## Creating and fitting the GloVe model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz9ajgpRaJ3j"
      },
      "source": [
        "Now we have a TCM matrix and can factorize it via the GloVe algorithm. We'll use the method `$new` to `GlobalVectors` to create our GloVe model. \n",
        "\n",
        "[Here](https://www.rdocumentation.org/packages/text2vec/versions/0.5.0/topics/GlobalVectors) is documentation for related functions and methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "0VG8YLfWaSuj",
        "outputId": "31fb15d7-7f0f-494a-a399-988bd65d6375"
      },
      "source": [
        "# Creating new GloVe model\n",
        "glove <- GlobalVectors$new(rank = 50, x_max = 10)\n",
        "\n",
        "# Checking GloVe methods\n",
        "glove"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<GloVe>\n",
              "  Public:\n",
              "    bias_i: NULL\n",
              "    bias_j: NULL\n",
              "    clone: function (deep = FALSE) \n",
              "    components: NULL\n",
              "    fit_transform: function (x, n_iter = 10L, convergence_tol = -1, n_threads = getOption(\"rsparse_omp_threads\", \n",
              "    get_history: function () \n",
              "    initialize: function (rank, x_max, learning_rate = 0.15, alpha = 0.75, lambda = 0, \n",
              "    shuffle: FALSE\n",
              "  Private:\n",
              "    alpha: 0.75\n",
              "    b_i: NULL\n",
              "    b_j: NULL\n",
              "    cost_history: \n",
              "    fitted: FALSE\n",
              "    glove_fitter: NULL\n",
              "    initial: NULL\n",
              "    lambda: 0\n",
              "    learning_rate: 0.15\n",
              "    rank: 50\n",
              "    w_i: NULL\n",
              "    w_j: NULL\n",
              "    x_max: 10"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SJK_pqTa29w"
      },
      "source": [
        "Note that you'll only be able to access the public methods.\n",
        "\n",
        "We can fit our model using `$fit_transform` to our `glove` variable. This may take several minutes to fit! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfTV2TWxbF_0",
        "outputId": "4025f646-b05a-4de2-a8e6-b318282f732a"
      },
      "source": [
        "# Fitting model\n",
        "wv_main <- glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO  [15:09:04.916] epoch 1, loss 0.1995 \n",
            "INFO  [15:09:08.927] epoch 2, loss 0.1301 \n",
            "INFO  [15:09:12.853] epoch 3, loss 0.1123 \n",
            "INFO  [15:09:16.770] epoch 4, loss 0.1014 \n",
            "INFO  [15:09:20.869] epoch 5, loss 0.0940 \n",
            "INFO  [15:09:24.724] epoch 6, loss 0.0886 \n",
            "INFO  [15:09:28.701] epoch 7, loss 0.0845 \n",
            "INFO  [15:09:32.886] epoch 8, loss 0.0813 \n",
            "INFO  [15:09:36.916] epoch 9, loss 0.0788 \n",
            "INFO  [15:09:40.967] epoch 10, loss 0.0766 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Yir-NHHXbPkU",
        "outputId": "17e92628-aa9c-42d6-89ed-626abe5b8b7c"
      },
      "source": [
        "# Checking dimensions\n",
        "dim(wv_main)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>14267</li><li>50</li></ol>\n"
            ],
            "text/markdown": "1. 14267\n2. 50\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 14267\n\\item 50\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 14267    50"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1-Vml6-bWkv"
      },
      "source": [
        "Note that model learns two sets of word vectors - **target** and **context**. We can think of our word of interest as the target in this environment, and all the other words as the context inside the window. For both, word vectors are learned. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "6q2rxO3ObaIU",
        "outputId": "6d7b52ff-d425-4dd0-f28b-e3aee82c097f"
      },
      "source": [
        "wv_context <- glove$components\n",
        "dim(wv_context)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>50</li><li>14267</li></ol>\n"
            ],
            "text/markdown": "1. 50\n2. 14267\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 50\n\\item 14267\n\\end{enumerate*}\n",
            "text/plain": [
              "[1]    50 14267"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ1MguP3biAE"
      },
      "source": [
        "While both of word-vectors matrices can be used as result, the creators recommends to average or take a sum of main and context vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ouOY85abilR"
      },
      "source": [
        "word_vectors <- wv_main + t(wv_context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFa079_Nyfwi"
      },
      "source": [
        "Here's a preview of the word vector matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hu2BV-Unbn--",
        "outputId": "cc6a56a7-2571-419f-dbc3-db981f3eb068"
      },
      "source": [
        "word_vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 14267 × 50 of type dbl</caption>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1837</th><td>-0.39605574</td><td>-0.02061349</td><td>-0.292531632</td><td>-0.49664328</td><td> 0.40783190</td><td> 0.851044482</td><td>-0.39696587</td><td>-0.27849286</td><td> 0.27692855</td><td> 0.570315492</td><td>⋯</td><td> 0.02241029</td><td>-0.57090033</td><td> 0.28269541</td><td>-0.709638090</td><td>-0.086413878</td><td>-0.436961662</td><td> 0.145116474</td><td>-0.13954358</td><td> 0.25301617</td><td> 0.22173371</td></tr>\n",
              "\t<tr><th scope=row>1841</th><td>-0.78071031</td><td> 0.65679353</td><td>-0.050169592</td><td>-0.44028214</td><td>-0.20699434</td><td> 0.545783287</td><td> 0.39728389</td><td> 0.25466314</td><td>-0.30259115</td><td> 0.170385098</td><td>⋯</td><td>-0.50191797</td><td>-0.35069074</td><td>-0.73582634</td><td> 0.092209004</td><td> 0.257043205</td><td>-0.581941567</td><td> 0.130368074</td><td>-0.01831186</td><td>-0.25701126</td><td>-0.47245659</td></tr>\n",
              "\t<tr><th scope=row>1881</th><td>-0.04010316</td><td>-0.10134305</td><td> 0.174045113</td><td>-0.56769329</td><td> 0.38104333</td><td> 0.343796140</td><td>-0.32884733</td><td>-0.19615921</td><td> 0.53750568</td><td> 0.134096555</td><td>⋯</td><td>-0.02316887</td><td>-0.58453249</td><td>-0.27100670</td><td>-0.472853220</td><td> 0.516830218</td><td>-0.339338338</td><td> 0.411892514</td><td> 0.97466772</td><td>-0.20560002</td><td> 0.68405675</td></tr>\n",
              "\t<tr><th scope=row>2005</th><td> 0.15787077</td><td> 0.71031086</td><td>-0.313078482</td><td>-0.61102057</td><td> 0.06495423</td><td> 0.371926194</td><td>-0.26801623</td><td>-0.86445663</td><td> 0.01352563</td><td>-0.171241807</td><td>⋯</td><td>-0.82696919</td><td> 0.16742684</td><td> 0.94363227</td><td>-0.590631081</td><td> 0.701651729</td><td> 0.077559228</td><td> 0.320037468</td><td> 0.10129664</td><td>-0.45100058</td><td> 0.20574856</td></tr>\n",
              "\t<tr><th scope=row>36</th><td> 0.19290715</td><td>-0.52808743</td><td>-0.423933116</td><td>-0.39152847</td><td> 0.43662860</td><td> 0.631995358</td><td> 0.09486522</td><td>-0.61107417</td><td> 0.50077352</td><td> 0.449836962</td><td>⋯</td><td>-0.33239024</td><td>-0.03064664</td><td>-0.48434318</td><td> 0.302711704</td><td>-0.035767930</td><td>-0.252195130</td><td> 0.004768143</td><td> 0.22295622</td><td>-0.16733290</td><td>-0.24080546</td></tr>\n",
              "\t<tr><th scope=row>38</th><td>-0.06613049</td><td> 0.59058818</td><td> 0.032173097</td><td> 0.44651101</td><td> 0.07999208</td><td>-0.254271109</td><td>-0.14656225</td><td>-0.30944246</td><td>-0.39287467</td><td>-0.108862124</td><td>⋯</td><td>-0.11765821</td><td>-0.75640020</td><td>-0.13223140</td><td>-0.325570517</td><td>-0.068856731</td><td>-0.689340349</td><td>-0.160185505</td><td> 0.34359886</td><td>-0.21039644</td><td> 0.01838548</td></tr>\n",
              "\t<tr><th scope=row>39</th><td>-0.13287689</td><td> 0.28692471</td><td>-0.003638418</td><td>-0.69530049</td><td> 0.43808501</td><td> 0.357472266</td><td> 0.25758576</td><td>-1.00892284</td><td>-0.01337265</td><td>-0.591372447</td><td>⋯</td><td>-0.53878903</td><td>-0.18590096</td><td> 0.24206018</td><td> 0.622937921</td><td>-0.183574815</td><td> 0.001330743</td><td>-0.091965269</td><td> 0.49289726</td><td>-0.26660598</td><td>-0.24322074</td></tr>\n",
              "\t<tr><th scope=row>52</th><td>-0.19872705</td><td>-0.20982830</td><td> 0.326931644</td><td>-0.49967092</td><td> 0.07001185</td><td>-0.105890798</td><td>-0.32086134</td><td> 0.13573098</td><td> 0.83251341</td><td> 0.306377047</td><td>⋯</td><td>-0.33254810</td><td> 0.12059476</td><td> 0.67515880</td><td> 0.240465878</td><td> 0.191630896</td><td>-0.099129157</td><td> 0.340868096</td><td>-0.12504141</td><td> 0.46920537</td><td> 0.53592542</td></tr>\n",
              "\t<tr><th scope=row>5â</th><td> 0.16738605</td><td> 0.43949076</td><td>-0.655501132</td><td> 0.07723001</td><td>-0.09199876</td><td>-0.034267178</td><td> 0.04048797</td><td>-0.17018697</td><td> 0.52170505</td><td> 0.603731582</td><td>⋯</td><td>-0.85178506</td><td>-0.30215470</td><td>-1.08300433</td><td>-0.460445616</td><td> 0.370161831</td><td>-0.519443597</td><td>-0.328239128</td><td> 0.69915124</td><td>-0.99653031</td><td>-0.42381830</td></tr>\n",
              "\t<tr><th scope=row>600</th><td>-0.18319427</td><td> 0.10173004</td><td> 0.267230059</td><td>-0.35690795</td><td> 0.43876954</td><td>-0.464610861</td><td> 0.49667306</td><td>-0.13993497</td><td> 0.41457057</td><td> 0.333022652</td><td>⋯</td><td>-0.49316066</td><td>-0.36144923</td><td> 0.25198538</td><td>-0.132789275</td><td>-0.021780580</td><td>-1.235329792</td><td> 0.112048058</td><td>-0.42602652</td><td> 0.31026247</td><td> 0.24424602</td></tr>\n",
              "\t<tr><th scope=row>abcdefg</th><td> 0.70735335</td><td> 0.03986606</td><td>-0.761714992</td><td>-0.80056051</td><td> 0.53046658</td><td> 0.813294633</td><td> 0.03168502</td><td>-0.83020056</td><td> 0.54576263</td><td> 0.382438103</td><td>⋯</td><td> 0.14297748</td><td> 0.58938418</td><td> 0.43381397</td><td>-0.266562818</td><td> 0.073584107</td><td> 0.101853044</td><td> 0.715129888</td><td> 0.35189552</td><td> 0.17785692</td><td>-0.61088021</td></tr>\n",
              "\t<tr><th scope=row>abroad</th><td>-0.08418838</td><td> 0.65628976</td><td>-0.406291943</td><td>-0.58331686</td><td> 0.48694521</td><td> 0.090336337</td><td> 0.55424315</td><td>-0.08964373</td><td> 0.08265955</td><td>-0.715753155</td><td>⋯</td><td> 0.19349736</td><td> 0.51979299</td><td> 0.60822378</td><td>-0.556848371</td><td>-0.139043952</td><td>-0.181291751</td><td> 0.589252357</td><td> 0.68893515</td><td>-0.29230629</td><td> 0.60544112</td></tr>\n",
              "\t<tr><th scope=row>abruptly</th><td>-0.75465511</td><td>-0.38653390</td><td>-0.569676230</td><td> 0.25027457</td><td> 0.18802824</td><td> 0.063757877</td><td>-0.18784266</td><td>-0.68724042</td><td> 0.81150829</td><td>-0.015416972</td><td>⋯</td><td>-0.29018679</td><td> 0.37914324</td><td>-0.56447501</td><td>-0.505668540</td><td> 0.335404427</td><td> 0.097786019</td><td>-0.388914190</td><td>-0.38557285</td><td> 0.10788126</td><td> 1.07054095</td></tr>\n",
              "\t<tr><th scope=row>absorbing</th><td>-0.50020257</td><td>-0.33860186</td><td>-0.777045595</td><td>-0.17992889</td><td>-0.58877689</td><td> 0.543358817</td><td>-0.33775998</td><td> 0.53389019</td><td>-0.38384163</td><td>-0.005132127</td><td>⋯</td><td>-0.42977623</td><td> 0.10758341</td><td>-0.26810107</td><td>-0.287205026</td><td>-0.312171091</td><td>-0.357957836</td><td> 0.167526479</td><td> 0.42167415</td><td>-0.15069332</td><td> 0.27986481</td></tr>\n",
              "\t<tr><th scope=row>accomplishments</th><td>-0.41715199</td><td> 0.11152296</td><td> 0.467634402</td><td> 0.32239662</td><td> 0.64448575</td><td> 0.289161685</td><td>-0.59506542</td><td> 0.07581241</td><td> 0.25354535</td><td>-0.042860936</td><td>⋯</td><td>-0.07269870</td><td>-0.48796473</td><td> 0.42911527</td><td> 0.296464644</td><td> 0.184565544</td><td> 0.242916970</td><td>-0.297316351</td><td>-0.02192918</td><td> 0.02381857</td><td> 1.01208497</td></tr>\n",
              "\t<tr><th scope=row>accused</th><td>-0.26991649</td><td> 0.03288589</td><td>-0.169466644</td><td> 0.52264325</td><td> 0.47107478</td><td> 0.354373016</td><td> 0.81422378</td><td>-0.41457847</td><td> 0.48387971</td><td> 0.522368365</td><td>⋯</td><td> 0.27525556</td><td>-0.73370105</td><td> 0.46510937</td><td> 0.603896149</td><td> 0.979517578</td><td>-0.196961453</td><td>-0.328864534</td><td>-0.58027338</td><td>-0.52653290</td><td> 0.45906009</td></tr>\n",
              "\t<tr><th scope=row>achievement</th><td> 0.39997079</td><td> 0.41026612</td><td>-0.341842703</td><td>-0.41467966</td><td>-0.31928564</td><td>-0.005160941</td><td> 0.36743120</td><td> 0.45298109</td><td> 0.70166833</td><td>-0.574403694</td><td>⋯</td><td>-0.47364188</td><td>-0.88646287</td><td> 0.26261689</td><td>-0.005924459</td><td> 0.660246560</td><td>-0.723082428</td><td>-0.149632690</td><td>-0.13746019</td><td>-0.06720847</td><td> 0.22176334</td></tr>\n",
              "\t<tr><th scope=row>acquired</th><td> 0.48017198</td><td> 0.19959144</td><td> 0.017158356</td><td>-0.21133543</td><td> 0.37107769</td><td>-0.148840688</td><td>-0.37196266</td><td>-0.13091843</td><td> 0.22530481</td><td>-0.029761000</td><td>⋯</td><td>-0.19462462</td><td>-0.03255923</td><td> 0.38051812</td><td>-0.439290708</td><td> 0.152316245</td><td>-0.474707575</td><td>-0.062887407</td><td> 0.03274934</td><td>-0.24725423</td><td> 0.24409540</td></tr>\n",
              "\t<tr><th scope=row>acres</th><td> 0.02621845</td><td> 0.20148900</td><td>-0.750877582</td><td> 0.23827775</td><td>-0.22556586</td><td>-0.082994703</td><td>-0.24850354</td><td>-0.02573336</td><td>-0.39737033</td><td>-0.686905678</td><td>⋯</td><td>-0.69901139</td><td>-0.45011647</td><td> 0.10235671</td><td>-0.256821371</td><td> 0.664099741</td><td> 0.234280279</td><td> 0.411153731</td><td> 0.39836951</td><td> 0.05258725</td><td>-0.32525395</td></tr>\n",
              "\t<tr><th scope=row>adams</th><td> 0.26623502</td><td> 1.01935488</td><td>-0.402454807</td><td>-0.22125943</td><td> 0.04596833</td><td> 0.067207560</td><td> 0.01206120</td><td>-0.35462093</td><td> 0.43541320</td><td>-0.379243165</td><td>⋯</td><td>-0.15590756</td><td>-0.57444189</td><td> 0.39395363</td><td> 0.366176218</td><td>-0.230617141</td><td>-0.443790956</td><td>-0.056869235</td><td> 0.19434052</td><td> 0.12382523</td><td> 0.04268834</td></tr>\n",
              "\t<tr><th scope=row>address</th><td>-0.10785436</td><td>-0.72239178</td><td>-0.014587346</td><td>-0.20541192</td><td> 0.06330883</td><td> 0.296418469</td><td>-0.01966000</td><td>-0.23844020</td><td> 0.60724074</td><td> 0.023561460</td><td>⋯</td><td> 0.08453997</td><td>-0.17504163</td><td> 0.79019792</td><td>-0.299842167</td><td> 0.005570082</td><td>-0.697858051</td><td> 0.967882308</td><td> 0.74267883</td><td>-0.23478951</td><td>-0.08638539</td></tr>\n",
              "\t<tr><th scope=row>adjust</th><td>-0.24831227</td><td> 0.30466502</td><td>-0.031096349</td><td>-0.04317058</td><td>-0.75828243</td><td> 0.366924123</td><td> 0.87349832</td><td>-0.68680689</td><td> 0.54500601</td><td>-0.343893322</td><td>⋯</td><td> 0.37090373</td><td>-0.01961698</td><td>-0.10056845</td><td>-0.161753172</td><td>-0.110804855</td><td>-0.080501837</td><td>-0.833086287</td><td> 0.18999399</td><td>-0.60048538</td><td> 0.52134227</td></tr>\n",
              "\t<tr><th scope=row>admires</th><td>-0.31729958</td><td> 0.44147387</td><td>-0.021398545</td><td> 0.12188454</td><td> 0.71702030</td><td>-0.419285741</td><td> 0.24584987</td><td> 0.22101916</td><td> 0.73687478</td><td>-0.558588185</td><td>⋯</td><td> 0.21235213</td><td>-0.07358972</td><td>-0.18027356</td><td> 0.750364485</td><td>-0.452595970</td><td>-0.414619174</td><td>-0.129256578</td><td>-0.12132017</td><td>-0.75372962</td><td> 0.85117923</td></tr>\n",
              "\t<tr><th scope=row>admits</th><td>-0.95281973</td><td> 0.12204455</td><td>-0.205021076</td><td> 0.17699643</td><td>-0.02412552</td><td> 0.549661586</td><td>-0.12190072</td><td>-0.29200957</td><td> 0.62044908</td><td>-0.391099078</td><td>⋯</td><td>-0.34380550</td><td>-0.32053749</td><td> 0.23215307</td><td>-0.496023417</td><td>-0.126543225</td><td>-0.613394837</td><td> 0.450257382</td><td> 0.47289568</td><td> 0.25420994</td><td>-0.19119665</td></tr>\n",
              "\t<tr><th scope=row>admitted</th><td> 0.18759031</td><td>-0.32369531</td><td>-0.938430479</td><td>-0.17741197</td><td>-0.47941412</td><td> 0.018157922</td><td> 0.32523834</td><td> 0.09749735</td><td> 0.86611433</td><td>-0.912179120</td><td>⋯</td><td>-0.56393874</td><td>-0.57302486</td><td> 0.03829672</td><td>-0.304267374</td><td> 0.291767974</td><td>-0.205611595</td><td> 0.173763380</td><td>-0.42971896</td><td>-0.54863718</td><td> 0.64700551</td></tr>\n",
              "\t<tr><th scope=row>adorn</th><td>-0.34048457</td><td>-0.53606391</td><td>-0.166729348</td><td>-0.04248947</td><td>-0.18723637</td><td>-0.421273007</td><td> 0.47392090</td><td>-0.12530631</td><td>-0.04284201</td><td> 0.243934259</td><td>⋯</td><td>-0.31114129</td><td>-0.17483361</td><td> 0.24586344</td><td> 0.737974531</td><td>-0.031574930</td><td>-0.756253249</td><td> 0.190669169</td><td>-0.43936459</td><td>-0.23082647</td><td> 0.11158748</td></tr>\n",
              "\t<tr><th scope=row>adrenalin</th><td>-0.20606141</td><td>-0.06482557</td><td> 0.295521636</td><td>-0.42895470</td><td> 0.03211840</td><td> 0.931785550</td><td>-0.55653735</td><td>-0.22255087</td><td> 0.62056343</td><td>-0.028575716</td><td>⋯</td><td>-0.12886131</td><td>-0.06061505</td><td>-0.67201540</td><td>-0.237778428</td><td> 0.292461247</td><td>-0.500908995</td><td>-0.029012521</td><td> 0.30806042</td><td> 0.25517963</td><td> 0.66449649</td></tr>\n",
              "\t<tr><th scope=row>adress</th><td> 0.78366920</td><td>-0.66037199</td><td>-0.794603296</td><td> 0.15212204</td><td>-0.06620859</td><td>-0.015640330</td><td>-0.10340023</td><td>-0.34022111</td><td>-0.14894531</td><td>-0.339456034</td><td>⋯</td><td>-0.54330462</td><td>-0.64177143</td><td> 0.21696370</td><td>-0.269471441</td><td> 0.175601141</td><td>-0.146203963</td><td> 0.245779959</td><td>-0.07227204</td><td>-0.52550125</td><td> 0.23499252</td></tr>\n",
              "\t<tr><th scope=row>adrift</th><td> 0.05843164</td><td>-0.58770716</td><td>-0.536694405</td><td>-0.65818455</td><td> 0.11936475</td><td>-0.065375977</td><td>-0.13477620</td><td> 0.09075554</td><td>-0.05782813</td><td>-0.445612478</td><td>⋯</td><td>-0.61756805</td><td>-0.16649287</td><td>-0.25527553</td><td> 0.555799185</td><td> 0.612421355</td><td>-0.834533678</td><td>-0.337302176</td><td> 0.22362290</td><td>-0.09379581</td><td> 0.11036846</td></tr>\n",
              "\t<tr><th scope=row>advanced</th><td> 0.56084521</td><td> 1.14434186</td><td>-0.385504901</td><td>-0.38727650</td><td> 0.20843386</td><td> 0.026028555</td><td> 0.63470848</td><td>-0.02157528</td><td> 0.64902722</td><td>-0.129253175</td><td>⋯</td><td>-0.10880402</td><td>-0.33665753</td><td> 0.16229289</td><td>-0.361123811</td><td>-0.306083825</td><td> 0.121554167</td><td> 0.802122076</td><td> 0.53786073</td><td>-0.87075226</td><td>-0.35036325</td></tr>\n",
              "\t<tr><th scope=row>⋮</th><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋱</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
              "\t<tr><th scope=row>your</th><td>-0.60564646</td><td>-0.62553236</td><td>1.24490314</td><td> 0.07566892</td><td> 0.06613355</td><td> 0.20188949</td><td>-0.409003665</td><td> 0.5676421848</td><td>-0.8230869</td><td>-0.699537680</td><td>⋯</td><td>1.2401653</td><td> 0.20952914</td><td>-0.893507520</td><td> 1.10687310</td><td>-0.02433232</td><td>1.1524797</td><td>-0.511380807</td><td>-0.31819395</td><td> 0.021489723</td><td> 0.26303308</td></tr>\n",
              "\t<tr><th scope=row>she</th><td> 1.25638190</td><td>-0.27417210</td><td>0.78292581</td><td> 0.64209586</td><td>-0.64827656</td><td> 0.47055208</td><td>-0.908629744</td><td>-0.4771845198</td><td>-0.5481247</td><td>-0.671014686</td><td>⋯</td><td>1.5116232</td><td>-0.63008666</td><td> 0.288729925</td><td>-0.09693033</td><td>-0.34142356</td><td>1.4555342</td><td>-0.502895692</td><td> 0.11742506</td><td>-0.016573978</td><td> 0.85044696</td></tr>\n",
              "\t<tr><th scope=row>love</th><td>-0.47156184</td><td>-0.41615846</td><td>0.58087619</td><td> 0.16058354</td><td>-0.43777360</td><td> 0.57852725</td><td> 0.479625783</td><td> 0.7178994202</td><td>-0.7162056</td><td> 0.181676720</td><td>⋯</td><td>1.3686970</td><td> 0.35205021</td><td> 0.242322126</td><td> 1.10917708</td><td>-0.83019695</td><td>1.6898825</td><td> 0.526807611</td><td>-0.03894946</td><td> 0.456800124</td><td>-0.89155241</td></tr>\n",
              "\t<tr><th scope=row>they</th><td>-0.35827850</td><td>-0.30550349</td><td>0.41978209</td><td>-0.90778265</td><td>-0.43329577</td><td> 0.49337017</td><td>-0.403912329</td><td> 0.2118026287</td><td>-0.3351952</td><td>-0.241848583</td><td>⋯</td><td>0.7515785</td><td> 0.56610648</td><td>-0.252167427</td><td> 0.65779104</td><td>-1.27628312</td><td>1.0787799</td><td>-0.648060347</td><td>-0.79852493</td><td> 0.551091693</td><td>-0.14490979</td></tr>\n",
              "\t<tr><th scope=row>all</th><td> 0.11720411</td><td>-0.74526340</td><td>0.20247411</td><td> 0.28941858</td><td>-0.92638975</td><td> 0.56686602</td><td>-0.172279828</td><td> 0.7941381581</td><td>-0.3109786</td><td>-0.614249410</td><td>⋯</td><td>0.6227535</td><td> 0.30527561</td><td>-0.126590147</td><td> 1.30020891</td><td>-0.08803566</td><td>0.8815118</td><td>-0.885388090</td><td>-0.67187065</td><td> 0.524857500</td><td>-0.40063246</td></tr>\n",
              "\t<tr><th scope=row>as</th><td>-0.11624688</td><td>-0.25615083</td><td>0.63723694</td><td>-0.15859595</td><td> 0.26664789</td><td> 0.03729823</td><td>-0.714140545</td><td> 0.3447922780</td><td>-1.2119347</td><td> 1.045180778</td><td>⋯</td><td>0.5350480</td><td>-0.09392316</td><td>-0.840557636</td><td> 0.12374327</td><td>-0.20070733</td><td>1.3998739</td><td>-0.623881007</td><td>-0.67044334</td><td> 0.337694542</td><td> 0.11456077</td></tr>\n",
              "\t<tr><th scope=row>for</th><td>-0.25557120</td><td>-1.22774622</td><td>0.36732031</td><td> 1.15379730</td><td>-0.81105382</td><td>-0.58278982</td><td>-0.007714816</td><td> 0.1867806799</td><td>-0.3772156</td><td>-0.327810962</td><td>⋯</td><td>0.8121711</td><td> 0.90376398</td><td> 0.223292297</td><td> 0.76098587</td><td> 0.20402389</td><td>1.0796115</td><td>-0.136541600</td><td>-0.84632118</td><td>-0.184942038</td><td>-0.51611075</td></tr>\n",
              "\t<tr><th scope=row>be</th><td>-0.14850891</td><td>-0.98468027</td><td>0.08846009</td><td> 0.24884741</td><td> 0.13636602</td><td> 0.13814258</td><td> 0.221039144</td><td>-0.3272235393</td><td>-1.0083243</td><td>-0.143189207</td><td>⋯</td><td>0.7192127</td><td> 0.65354656</td><td>-0.163325842</td><td> 0.87071768</td><td>-0.36572406</td><td>1.5257384</td><td> 0.333888251</td><td>-0.39944828</td><td> 0.806497218</td><td> 0.14134930</td></tr>\n",
              "\t<tr><th scope=row>when</th><td> 0.57533352</td><td>-0.53009848</td><td>1.01794899</td><td> 0.72600962</td><td>-0.68010104</td><td> 0.21665274</td><td>-0.319655988</td><td> 0.0009623705</td><td>-0.5837773</td><td> 0.008194372</td><td>⋯</td><td>1.1439938</td><td> 0.33273533</td><td> 0.007094395</td><td> 0.53755694</td><td>-0.34664721</td><td>1.2436099</td><td>-0.448986056</td><td>-0.60906069</td><td> 0.226135761</td><td> 0.11392773</td></tr>\n",
              "\t<tr><th scope=row>with</th><td> 0.17133929</td><td>-0.17838819</td><td>0.58251350</td><td> 0.11253254</td><td>-0.38398995</td><td>-0.51119930</td><td>-0.053675252</td><td> 0.6514033853</td><td>-0.4789235</td><td> 0.501062456</td><td>⋯</td><td>1.1618845</td><td>-0.22637164</td><td>-0.363779317</td><td> 1.00956877</td><td>-0.65822579</td><td>0.6530590</td><td>-1.009942032</td><td>-0.70892681</td><td> 0.265583915</td><td>-0.69986409</td></tr>\n",
              "\t<tr><th scope=row>so</th><td> 0.05879581</td><td>-0.53859237</td><td>0.43241685</td><td>-0.19798659</td><td> 0.37991846</td><td> 0.66512085</td><td>-0.356972696</td><td> 0.0439883445</td><td>-0.6670437</td><td>-0.047874663</td><td>⋯</td><td>1.0497711</td><td> 0.42287794</td><td>-0.634407802</td><td> 0.66041415</td><td>-0.25366041</td><td>1.9301640</td><td>-0.401711816</td><td>-0.60208501</td><td> 0.260621312</td><td> 0.06687380</td></tr>\n",
              "\t<tr><th scope=row>on</th><td> 0.45676682</td><td> 0.14578002</td><td>0.71469450</td><td> 0.69460988</td><td>-0.55329123</td><td>-0.83893077</td><td>-0.333487314</td><td> 0.0208520019</td><td>-1.1300300</td><td>-1.062147051</td><td>⋯</td><td>1.0803135</td><td> 0.78212691</td><td>-1.114261244</td><td> 0.68217380</td><td>-0.01680861</td><td>0.7734562</td><td>-1.472068964</td><td>-0.97036547</td><td> 0.680163351</td><td>-0.55308657</td></tr>\n",
              "\t<tr><th scope=row>was</th><td> 1.26103268</td><td>-0.87521985</td><td>0.42591786</td><td> 1.44752023</td><td>-0.56292737</td><td>-0.53824005</td><td> 0.700908841</td><td>-1.0281724581</td><td>-0.7886144</td><td>-0.963085681</td><td>⋯</td><td>1.0668965</td><td> 0.17199371</td><td>-0.151018057</td><td> 0.33248939</td><td> 0.01220992</td><td>1.2473095</td><td>-0.494631285</td><td>-0.82375822</td><td> 0.323448808</td><td> 0.44993179</td></tr>\n",
              "\t<tr><th scope=row>but</th><td> 0.07047070</td><td>-0.42080162</td><td>0.25806188</td><td> 0.40316479</td><td>-0.59542069</td><td> 0.28235011</td><td>-0.349787195</td><td> 0.1637144872</td><td>-0.8339218</td><td>-0.355345842</td><td>⋯</td><td>0.9462293</td><td> 0.24685036</td><td> 0.070578389</td><td> 0.55861198</td><td>-0.22122495</td><td>1.4829625</td><td> 0.004946889</td><td>-0.48550488</td><td> 0.359658026</td><td> 0.21088970</td></tr>\n",
              "\t<tr><th scope=row>he</th><td> 1.25229438</td><td> 0.12016153</td><td>0.63392055</td><td> 0.84683830</td><td>-0.90478847</td><td>-0.35824703</td><td>-0.677012685</td><td>-0.7382110010</td><td>-0.6369223</td><td>-0.589857265</td><td>⋯</td><td>1.5686786</td><td>-0.46246393</td><td> 0.553926860</td><td>-0.25155399</td><td>-0.22799796</td><td>1.4709812</td><td>-0.989057028</td><td>-0.09478956</td><td> 0.002057899</td><td> 1.02377057</td></tr>\n",
              "\t<tr><th scope=row>like</th><td>-0.04622273</td><td> 0.29959450</td><td>0.92314664</td><td>-0.26548874</td><td>-0.47171732</td><td> 0.58351138</td><td>-0.604306667</td><td> 0.3800892986</td><td>-1.1940816</td><td> 0.825574944</td><td>⋯</td><td>0.8058586</td><td>-0.23305871</td><td>-0.226500325</td><td> 0.56541311</td><td>-0.13815864</td><td>1.4804062</td><td>-0.317186583</td><td>-0.70301863</td><td> 0.960619367</td><td>-0.35390149</td></tr>\n",
              "\t<tr><th scope=row>that</th><td> 0.29461876</td><td>-0.87759121</td><td>0.05826057</td><td> 0.37605319</td><td>-1.05454057</td><td> 0.35491667</td><td> 0.357027236</td><td>-0.0322892948</td><td>-0.8261352</td><td>-0.416421353</td><td>⋯</td><td>0.8274029</td><td> 0.20535042</td><td>-0.163242226</td><td> 0.43860085</td><td>-0.51347366</td><td>1.6675639</td><td>-0.123437992</td><td>-0.38485505</td><td> 0.190445567</td><td> 0.02113333</td></tr>\n",
              "\t<tr><th scope=row>are</th><td>-1.01009041</td><td>-0.38996589</td><td>0.78437092</td><td>-1.20336579</td><td> 0.32266309</td><td> 1.77786924</td><td>-0.198341769</td><td> 0.9877242543</td><td>-0.9196615</td><td> 0.262582247</td><td>⋯</td><td>0.8245676</td><td> 0.72205429</td><td>-0.611866868</td><td> 2.17995070</td><td>-0.43209494</td><td>0.7141540</td><td>-0.310214277</td><td>-0.62570673</td><td> 0.790375902</td><td>-0.38930963</td></tr>\n",
              "\t<tr><th scope=row>me</th><td> 0.91295703</td><td>-0.67142812</td><td>0.89832726</td><td> 0.72298262</td><td>-0.33188895</td><td> 0.18575597</td><td> 0.095386702</td><td> 0.3446390058</td><td>-0.5322870</td><td>-0.051038622</td><td>⋯</td><td>0.9874224</td><td> 0.59983195</td><td> 0.634230040</td><td> 0.82089915</td><td>-1.02258828</td><td>1.7741451</td><td> 0.354590480</td><td> 0.63372518</td><td> 0.275596054</td><td> 0.36610939</td></tr>\n",
              "\t<tr><th scope=row>of</th><td> 0.02042433</td><td>-0.61814765</td><td>0.41187393</td><td> 0.78508502</td><td>-0.62855862</td><td>-0.17142417</td><td>-0.196826144</td><td> 1.0146473032</td><td>-0.5444330</td><td> 0.653661660</td><td>⋯</td><td>0.7545896</td><td> 0.11419678</td><td> 0.319663670</td><td> 1.11015049</td><td> 0.70160431</td><td>0.6867089</td><td>-0.244857004</td><td>-0.37555101</td><td>-0.183021007</td><td>-1.45380483</td></tr>\n",
              "\t<tr><th scope=row>in</th><td> 0.29488392</td><td>-0.90998782</td><td>0.58842261</td><td> 0.49775559</td><td>-0.56231897</td><td>-0.29213216</td><td>-0.099918729</td><td> 0.8558565963</td><td>-0.9014696</td><td>-0.058092618</td><td>⋯</td><td>0.7614794</td><td> 0.93653439</td><td>-0.324843002</td><td> 0.67796794</td><td> 0.43627938</td><td>0.7676684</td><td>-1.628720396</td><td>-0.29710615</td><td> 0.173605579</td><td>-1.31176620</td></tr>\n",
              "\t<tr><th scope=row>it</th><td> 0.88038745</td><td>-0.27740931</td><td>0.33769112</td><td> 0.83809891</td><td>-1.06788656</td><td> 0.65374539</td><td>-0.413026174</td><td> 0.1071945725</td><td>-0.7656125</td><td>-0.031481317</td><td>⋯</td><td>0.9401578</td><td>-0.01717349</td><td>-0.185548671</td><td> 0.40555803</td><td> 0.26815605</td><td>1.6548084</td><td>-0.330136184</td><td>-0.38485989</td><td> 0.355766796</td><td>-0.56673340</td></tr>\n",
              "\t<tr><th scope=row>you</th><td> 0.02921793</td><td>-0.69956947</td><td>0.65619414</td><td> 0.19990306</td><td>-0.55782712</td><td> 0.23775012</td><td> 0.557279978</td><td> 0.2540646605</td><td>-0.6562819</td><td>-0.102652868</td><td>⋯</td><td>0.9707404</td><td> 1.01878699</td><td> 0.051109162</td><td> 0.56709298</td><td>-1.09163977</td><td>1.8576337</td><td> 0.325543294</td><td>-0.27147131</td><td> 0.545372987</td><td>-0.13519577</td></tr>\n",
              "\t<tr><th scope=row>my</th><td>-0.36326315</td><td> 0.13292878</td><td>1.43764062</td><td> 0.96306913</td><td>-0.14513789</td><td> 0.09596304</td><td>-0.681646435</td><td> 0.4329776124</td><td>-0.7721462</td><td>-1.049417196</td><td>⋯</td><td>1.9199913</td><td>-0.25758989</td><td>-0.724352957</td><td> 0.93463658</td><td>-0.11169862</td><td>1.5346297</td><td>-0.788939348</td><td>-0.27372689</td><td>-0.436236316</td><td> 0.39523997</td></tr>\n",
              "\t<tr><th scope=row>is</th><td>-0.26110188</td><td>-0.10388824</td><td>0.54050263</td><td> 0.82070089</td><td>-0.47990125</td><td> 0.10403162</td><td>-0.047889726</td><td> 0.3543069185</td><td>-1.4300532</td><td> 0.220242903</td><td>⋯</td><td>1.4188999</td><td> 0.05045011</td><td> 0.181550703</td><td> 0.78262696</td><td> 1.31974635</td><td>1.8564849</td><td> 0.246831066</td><td>-0.62006157</td><td> 0.448739600</td><td>-0.52222979</td></tr>\n",
              "\t<tr><th scope=row>to</th><td> 0.04527890</td><td>-0.34062380</td><td>0.17724608</td><td>-0.03262842</td><td>-1.08603362</td><td>-0.69119638</td><td> 0.153098458</td><td> 0.3512644683</td><td>-0.6422789</td><td>-0.708777481</td><td>⋯</td><td>0.9725228</td><td> 0.49056372</td><td> 0.078905810</td><td> 0.31924556</td><td>-0.67267930</td><td>1.7620503</td><td>-0.505294399</td><td>-0.34906607</td><td> 0.126604838</td><td>-0.54660212</td></tr>\n",
              "\t<tr><th scope=row>and</th><td> 0.49708222</td><td>-0.05538622</td><td>0.64361148</td><td>-0.17765136</td><td>-0.27773266</td><td> 0.02712476</td><td>-0.705487112</td><td> 0.6719607646</td><td>-0.7141925</td><td> 0.155614203</td><td>⋯</td><td>1.1783046</td><td>-0.18493637</td><td>-0.238077333</td><td> 0.81325745</td><td>-0.25105040</td><td>0.9476438</td><td>-0.762718670</td><td>-0.34686658</td><td> 0.076044470</td><td>-0.30018538</td></tr>\n",
              "\t<tr><th scope=row>a</th><td> 0.03933321</td><td> 0.36935742</td><td>0.47268360</td><td> 1.44602852</td><td>-0.31805893</td><td>-1.72934639</td><td> 0.614659565</td><td> 0.2250980756</td><td>-1.3363209</td><td> 0.244922024</td><td>⋯</td><td>1.1248268</td><td> 0.02815606</td><td>-0.455767982</td><td>-0.91003319</td><td>-0.21015546</td><td>1.3258001</td><td> 0.041061325</td><td>-1.26043371</td><td> 0.094672523</td><td>-0.58245419</td></tr>\n",
              "\t<tr><th scope=row>the</th><td> 0.29750320</td><td>-0.58607189</td><td>0.22902895</td><td> 0.34357388</td><td>-1.07724707</td><td>-0.44501136</td><td> 0.049346221</td><td> 0.7785464956</td><td>-0.6021630</td><td> 0.141874414</td><td>⋯</td><td>0.7296454</td><td> 0.98887298</td><td>-0.277144371</td><td> 0.13211323</td><td> 0.45814443</td><td>0.9946228</td><td>-1.611777582</td><td> 0.13637256</td><td> 0.509871545</td><td>-1.10640435</td></tr>\n",
              "\t<tr><th scope=row>i</th><td> 0.47940454</td><td>-0.16366432</td><td>0.70160325</td><td> 0.79155248</td><td>-0.82320585</td><td>-0.33732812</td><td> 0.736717288</td><td>-0.1692837726</td><td>-0.5418732</td><td>-0.138433952</td><td>⋯</td><td>1.1956301</td><td> 0.88732448</td><td>-0.492154390</td><td> 0.44235429</td><td>-0.87338609</td><td>1.5814970</td><td>-0.295314993</td><td>-0.63478018</td><td> 0.355809566</td><td>-0.27658641</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA matrix: 14267 × 50 of type dbl\n\n| 1837 | -0.39605574 | -0.02061349 | -0.292531632 | -0.49664328 |  0.40783190 |  0.851044482 | -0.39696587 | -0.27849286 |  0.27692855 |  0.570315492 | ⋯ |  0.02241029 | -0.57090033 |  0.28269541 | -0.709638090 | -0.086413878 | -0.436961662 |  0.145116474 | -0.13954358 |  0.25301617 |  0.22173371 |\n| 1841 | -0.78071031 |  0.65679353 | -0.050169592 | -0.44028214 | -0.20699434 |  0.545783287 |  0.39728389 |  0.25466314 | -0.30259115 |  0.170385098 | ⋯ | -0.50191797 | -0.35069074 | -0.73582634 |  0.092209004 |  0.257043205 | -0.581941567 |  0.130368074 | -0.01831186 | -0.25701126 | -0.47245659 |\n| 1881 | -0.04010316 | -0.10134305 |  0.174045113 | -0.56769329 |  0.38104333 |  0.343796140 | -0.32884733 | -0.19615921 |  0.53750568 |  0.134096555 | ⋯ | -0.02316887 | -0.58453249 | -0.27100670 | -0.472853220 |  0.516830218 | -0.339338338 |  0.411892514 |  0.97466772 | -0.20560002 |  0.68405675 |\n| 2005 |  0.15787077 |  0.71031086 | -0.313078482 | -0.61102057 |  0.06495423 |  0.371926194 | -0.26801623 | -0.86445663 |  0.01352563 | -0.171241807 | ⋯ | -0.82696919 |  0.16742684 |  0.94363227 | -0.590631081 |  0.701651729 |  0.077559228 |  0.320037468 |  0.10129664 | -0.45100058 |  0.20574856 |\n| 36 |  0.19290715 | -0.52808743 | -0.423933116 | -0.39152847 |  0.43662860 |  0.631995358 |  0.09486522 | -0.61107417 |  0.50077352 |  0.449836962 | ⋯ | -0.33239024 | -0.03064664 | -0.48434318 |  0.302711704 | -0.035767930 | -0.252195130 |  0.004768143 |  0.22295622 | -0.16733290 | -0.24080546 |\n| 38 | -0.06613049 |  0.59058818 |  0.032173097 |  0.44651101 |  0.07999208 | -0.254271109 | -0.14656225 | -0.30944246 | -0.39287467 | -0.108862124 | ⋯ | -0.11765821 | -0.75640020 | -0.13223140 | -0.325570517 | -0.068856731 | -0.689340349 | -0.160185505 |  0.34359886 | -0.21039644 |  0.01838548 |\n| 39 | -0.13287689 |  0.28692471 | -0.003638418 | -0.69530049 |  0.43808501 |  0.357472266 |  0.25758576 | -1.00892284 | -0.01337265 | -0.591372447 | ⋯ | -0.53878903 | -0.18590096 |  0.24206018 |  0.622937921 | -0.183574815 |  0.001330743 | -0.091965269 |  0.49289726 | -0.26660598 | -0.24322074 |\n| 52 | -0.19872705 | -0.20982830 |  0.326931644 | -0.49967092 |  0.07001185 | -0.105890798 | -0.32086134 |  0.13573098 |  0.83251341 |  0.306377047 | ⋯ | -0.33254810 |  0.12059476 |  0.67515880 |  0.240465878 |  0.191630896 | -0.099129157 |  0.340868096 | -0.12504141 |  0.46920537 |  0.53592542 |\n| 5â |  0.16738605 |  0.43949076 | -0.655501132 |  0.07723001 | -0.09199876 | -0.034267178 |  0.04048797 | -0.17018697 |  0.52170505 |  0.603731582 | ⋯ | -0.85178506 | -0.30215470 | -1.08300433 | -0.460445616 |  0.370161831 | -0.519443597 | -0.328239128 |  0.69915124 | -0.99653031 | -0.42381830 |\n| 600 | -0.18319427 |  0.10173004 |  0.267230059 | -0.35690795 |  0.43876954 | -0.464610861 |  0.49667306 | -0.13993497 |  0.41457057 |  0.333022652 | ⋯ | -0.49316066 | -0.36144923 |  0.25198538 | -0.132789275 | -0.021780580 | -1.235329792 |  0.112048058 | -0.42602652 |  0.31026247 |  0.24424602 |\n| abcdefg |  0.70735335 |  0.03986606 | -0.761714992 | -0.80056051 |  0.53046658 |  0.813294633 |  0.03168502 | -0.83020056 |  0.54576263 |  0.382438103 | ⋯ |  0.14297748 |  0.58938418 |  0.43381397 | -0.266562818 |  0.073584107 |  0.101853044 |  0.715129888 |  0.35189552 |  0.17785692 | -0.61088021 |\n| abroad | -0.08418838 |  0.65628976 | -0.406291943 | -0.58331686 |  0.48694521 |  0.090336337 |  0.55424315 | -0.08964373 |  0.08265955 | -0.715753155 | ⋯ |  0.19349736 |  0.51979299 |  0.60822378 | -0.556848371 | -0.139043952 | -0.181291751 |  0.589252357 |  0.68893515 | -0.29230629 |  0.60544112 |\n| abruptly | -0.75465511 | -0.38653390 | -0.569676230 |  0.25027457 |  0.18802824 |  0.063757877 | -0.18784266 | -0.68724042 |  0.81150829 | -0.015416972 | ⋯ | -0.29018679 |  0.37914324 | -0.56447501 | -0.505668540 |  0.335404427 |  0.097786019 | -0.388914190 | -0.38557285 |  0.10788126 |  1.07054095 |\n| absorbing | -0.50020257 | -0.33860186 | -0.777045595 | -0.17992889 | -0.58877689 |  0.543358817 | -0.33775998 |  0.53389019 | -0.38384163 | -0.005132127 | ⋯ | -0.42977623 |  0.10758341 | -0.26810107 | -0.287205026 | -0.312171091 | -0.357957836 |  0.167526479 |  0.42167415 | -0.15069332 |  0.27986481 |\n| accomplishments | -0.41715199 |  0.11152296 |  0.467634402 |  0.32239662 |  0.64448575 |  0.289161685 | -0.59506542 |  0.07581241 |  0.25354535 | -0.042860936 | ⋯ | -0.07269870 | -0.48796473 |  0.42911527 |  0.296464644 |  0.184565544 |  0.242916970 | -0.297316351 | -0.02192918 |  0.02381857 |  1.01208497 |\n| accused | -0.26991649 |  0.03288589 | -0.169466644 |  0.52264325 |  0.47107478 |  0.354373016 |  0.81422378 | -0.41457847 |  0.48387971 |  0.522368365 | ⋯ |  0.27525556 | -0.73370105 |  0.46510937 |  0.603896149 |  0.979517578 | -0.196961453 | -0.328864534 | -0.58027338 | -0.52653290 |  0.45906009 |\n| achievement |  0.39997079 |  0.41026612 | -0.341842703 | -0.41467966 | -0.31928564 | -0.005160941 |  0.36743120 |  0.45298109 |  0.70166833 | -0.574403694 | ⋯ | -0.47364188 | -0.88646287 |  0.26261689 | -0.005924459 |  0.660246560 | -0.723082428 | -0.149632690 | -0.13746019 | -0.06720847 |  0.22176334 |\n| acquired |  0.48017198 |  0.19959144 |  0.017158356 | -0.21133543 |  0.37107769 | -0.148840688 | -0.37196266 | -0.13091843 |  0.22530481 | -0.029761000 | ⋯ | -0.19462462 | -0.03255923 |  0.38051812 | -0.439290708 |  0.152316245 | -0.474707575 | -0.062887407 |  0.03274934 | -0.24725423 |  0.24409540 |\n| acres |  0.02621845 |  0.20148900 | -0.750877582 |  0.23827775 | -0.22556586 | -0.082994703 | -0.24850354 | -0.02573336 | -0.39737033 | -0.686905678 | ⋯ | -0.69901139 | -0.45011647 |  0.10235671 | -0.256821371 |  0.664099741 |  0.234280279 |  0.411153731 |  0.39836951 |  0.05258725 | -0.32525395 |\n| adams |  0.26623502 |  1.01935488 | -0.402454807 | -0.22125943 |  0.04596833 |  0.067207560 |  0.01206120 | -0.35462093 |  0.43541320 | -0.379243165 | ⋯ | -0.15590756 | -0.57444189 |  0.39395363 |  0.366176218 | -0.230617141 | -0.443790956 | -0.056869235 |  0.19434052 |  0.12382523 |  0.04268834 |\n| address | -0.10785436 | -0.72239178 | -0.014587346 | -0.20541192 |  0.06330883 |  0.296418469 | -0.01966000 | -0.23844020 |  0.60724074 |  0.023561460 | ⋯ |  0.08453997 | -0.17504163 |  0.79019792 | -0.299842167 |  0.005570082 | -0.697858051 |  0.967882308 |  0.74267883 | -0.23478951 | -0.08638539 |\n| adjust | -0.24831227 |  0.30466502 | -0.031096349 | -0.04317058 | -0.75828243 |  0.366924123 |  0.87349832 | -0.68680689 |  0.54500601 | -0.343893322 | ⋯ |  0.37090373 | -0.01961698 | -0.10056845 | -0.161753172 | -0.110804855 | -0.080501837 | -0.833086287 |  0.18999399 | -0.60048538 |  0.52134227 |\n| admires | -0.31729958 |  0.44147387 | -0.021398545 |  0.12188454 |  0.71702030 | -0.419285741 |  0.24584987 |  0.22101916 |  0.73687478 | -0.558588185 | ⋯ |  0.21235213 | -0.07358972 | -0.18027356 |  0.750364485 | -0.452595970 | -0.414619174 | -0.129256578 | -0.12132017 | -0.75372962 |  0.85117923 |\n| admits | -0.95281973 |  0.12204455 | -0.205021076 |  0.17699643 | -0.02412552 |  0.549661586 | -0.12190072 | -0.29200957 |  0.62044908 | -0.391099078 | ⋯ | -0.34380550 | -0.32053749 |  0.23215307 | -0.496023417 | -0.126543225 | -0.613394837 |  0.450257382 |  0.47289568 |  0.25420994 | -0.19119665 |\n| admitted |  0.18759031 | -0.32369531 | -0.938430479 | -0.17741197 | -0.47941412 |  0.018157922 |  0.32523834 |  0.09749735 |  0.86611433 | -0.912179120 | ⋯ | -0.56393874 | -0.57302486 |  0.03829672 | -0.304267374 |  0.291767974 | -0.205611595 |  0.173763380 | -0.42971896 | -0.54863718 |  0.64700551 |\n| adorn | -0.34048457 | -0.53606391 | -0.166729348 | -0.04248947 | -0.18723637 | -0.421273007 |  0.47392090 | -0.12530631 | -0.04284201 |  0.243934259 | ⋯ | -0.31114129 | -0.17483361 |  0.24586344 |  0.737974531 | -0.031574930 | -0.756253249 |  0.190669169 | -0.43936459 | -0.23082647 |  0.11158748 |\n| adrenalin | -0.20606141 | -0.06482557 |  0.295521636 | -0.42895470 |  0.03211840 |  0.931785550 | -0.55653735 | -0.22255087 |  0.62056343 | -0.028575716 | ⋯ | -0.12886131 | -0.06061505 | -0.67201540 | -0.237778428 |  0.292461247 | -0.500908995 | -0.029012521 |  0.30806042 |  0.25517963 |  0.66449649 |\n| adress |  0.78366920 | -0.66037199 | -0.794603296 |  0.15212204 | -0.06620859 | -0.015640330 | -0.10340023 | -0.34022111 | -0.14894531 | -0.339456034 | ⋯ | -0.54330462 | -0.64177143 |  0.21696370 | -0.269471441 |  0.175601141 | -0.146203963 |  0.245779959 | -0.07227204 | -0.52550125 |  0.23499252 |\n| adrift |  0.05843164 | -0.58770716 | -0.536694405 | -0.65818455 |  0.11936475 | -0.065375977 | -0.13477620 |  0.09075554 | -0.05782813 | -0.445612478 | ⋯ | -0.61756805 | -0.16649287 | -0.25527553 |  0.555799185 |  0.612421355 | -0.834533678 | -0.337302176 |  0.22362290 | -0.09379581 |  0.11036846 |\n| advanced |  0.56084521 |  1.14434186 | -0.385504901 | -0.38727650 |  0.20843386 |  0.026028555 |  0.63470848 | -0.02157528 |  0.64902722 | -0.129253175 | ⋯ | -0.10880402 | -0.33665753 |  0.16229289 | -0.361123811 | -0.306083825 |  0.121554167 |  0.802122076 |  0.53786073 | -0.87075226 | -0.35036325 |\n| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋱ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n| your | -0.60564646 | -0.62553236 | 1.24490314 |  0.07566892 |  0.06613355 |  0.20188949 | -0.409003665 |  0.5676421848 | -0.8230869 | -0.699537680 | ⋯ | 1.2401653 |  0.20952914 | -0.893507520 |  1.10687310 | -0.02433232 | 1.1524797 | -0.511380807 | -0.31819395 |  0.021489723 |  0.26303308 |\n| she |  1.25638190 | -0.27417210 | 0.78292581 |  0.64209586 | -0.64827656 |  0.47055208 | -0.908629744 | -0.4771845198 | -0.5481247 | -0.671014686 | ⋯ | 1.5116232 | -0.63008666 |  0.288729925 | -0.09693033 | -0.34142356 | 1.4555342 | -0.502895692 |  0.11742506 | -0.016573978 |  0.85044696 |\n| love | -0.47156184 | -0.41615846 | 0.58087619 |  0.16058354 | -0.43777360 |  0.57852725 |  0.479625783 |  0.7178994202 | -0.7162056 |  0.181676720 | ⋯ | 1.3686970 |  0.35205021 |  0.242322126 |  1.10917708 | -0.83019695 | 1.6898825 |  0.526807611 | -0.03894946 |  0.456800124 | -0.89155241 |\n| they | -0.35827850 | -0.30550349 | 0.41978209 | -0.90778265 | -0.43329577 |  0.49337017 | -0.403912329 |  0.2118026287 | -0.3351952 | -0.241848583 | ⋯ | 0.7515785 |  0.56610648 | -0.252167427 |  0.65779104 | -1.27628312 | 1.0787799 | -0.648060347 | -0.79852493 |  0.551091693 | -0.14490979 |\n| all |  0.11720411 | -0.74526340 | 0.20247411 |  0.28941858 | -0.92638975 |  0.56686602 | -0.172279828 |  0.7941381581 | -0.3109786 | -0.614249410 | ⋯ | 0.6227535 |  0.30527561 | -0.126590147 |  1.30020891 | -0.08803566 | 0.8815118 | -0.885388090 | -0.67187065 |  0.524857500 | -0.40063246 |\n| as | -0.11624688 | -0.25615083 | 0.63723694 | -0.15859595 |  0.26664789 |  0.03729823 | -0.714140545 |  0.3447922780 | -1.2119347 |  1.045180778 | ⋯ | 0.5350480 | -0.09392316 | -0.840557636 |  0.12374327 | -0.20070733 | 1.3998739 | -0.623881007 | -0.67044334 |  0.337694542 |  0.11456077 |\n| for | -0.25557120 | -1.22774622 | 0.36732031 |  1.15379730 | -0.81105382 | -0.58278982 | -0.007714816 |  0.1867806799 | -0.3772156 | -0.327810962 | ⋯ | 0.8121711 |  0.90376398 |  0.223292297 |  0.76098587 |  0.20402389 | 1.0796115 | -0.136541600 | -0.84632118 | -0.184942038 | -0.51611075 |\n| be | -0.14850891 | -0.98468027 | 0.08846009 |  0.24884741 |  0.13636602 |  0.13814258 |  0.221039144 | -0.3272235393 | -1.0083243 | -0.143189207 | ⋯ | 0.7192127 |  0.65354656 | -0.163325842 |  0.87071768 | -0.36572406 | 1.5257384 |  0.333888251 | -0.39944828 |  0.806497218 |  0.14134930 |\n| when |  0.57533352 | -0.53009848 | 1.01794899 |  0.72600962 | -0.68010104 |  0.21665274 | -0.319655988 |  0.0009623705 | -0.5837773 |  0.008194372 | ⋯ | 1.1439938 |  0.33273533 |  0.007094395 |  0.53755694 | -0.34664721 | 1.2436099 | -0.448986056 | -0.60906069 |  0.226135761 |  0.11392773 |\n| with |  0.17133929 | -0.17838819 | 0.58251350 |  0.11253254 | -0.38398995 | -0.51119930 | -0.053675252 |  0.6514033853 | -0.4789235 |  0.501062456 | ⋯ | 1.1618845 | -0.22637164 | -0.363779317 |  1.00956877 | -0.65822579 | 0.6530590 | -1.009942032 | -0.70892681 |  0.265583915 | -0.69986409 |\n| so |  0.05879581 | -0.53859237 | 0.43241685 | -0.19798659 |  0.37991846 |  0.66512085 | -0.356972696 |  0.0439883445 | -0.6670437 | -0.047874663 | ⋯ | 1.0497711 |  0.42287794 | -0.634407802 |  0.66041415 | -0.25366041 | 1.9301640 | -0.401711816 | -0.60208501 |  0.260621312 |  0.06687380 |\n| on |  0.45676682 |  0.14578002 | 0.71469450 |  0.69460988 | -0.55329123 | -0.83893077 | -0.333487314 |  0.0208520019 | -1.1300300 | -1.062147051 | ⋯ | 1.0803135 |  0.78212691 | -1.114261244 |  0.68217380 | -0.01680861 | 0.7734562 | -1.472068964 | -0.97036547 |  0.680163351 | -0.55308657 |\n| was |  1.26103268 | -0.87521985 | 0.42591786 |  1.44752023 | -0.56292737 | -0.53824005 |  0.700908841 | -1.0281724581 | -0.7886144 | -0.963085681 | ⋯ | 1.0668965 |  0.17199371 | -0.151018057 |  0.33248939 |  0.01220992 | 1.2473095 | -0.494631285 | -0.82375822 |  0.323448808 |  0.44993179 |\n| but |  0.07047070 | -0.42080162 | 0.25806188 |  0.40316479 | -0.59542069 |  0.28235011 | -0.349787195 |  0.1637144872 | -0.8339218 | -0.355345842 | ⋯ | 0.9462293 |  0.24685036 |  0.070578389 |  0.55861198 | -0.22122495 | 1.4829625 |  0.004946889 | -0.48550488 |  0.359658026 |  0.21088970 |\n| he |  1.25229438 |  0.12016153 | 0.63392055 |  0.84683830 | -0.90478847 | -0.35824703 | -0.677012685 | -0.7382110010 | -0.6369223 | -0.589857265 | ⋯ | 1.5686786 | -0.46246393 |  0.553926860 | -0.25155399 | -0.22799796 | 1.4709812 | -0.989057028 | -0.09478956 |  0.002057899 |  1.02377057 |\n| like | -0.04622273 |  0.29959450 | 0.92314664 | -0.26548874 | -0.47171732 |  0.58351138 | -0.604306667 |  0.3800892986 | -1.1940816 |  0.825574944 | ⋯ | 0.8058586 | -0.23305871 | -0.226500325 |  0.56541311 | -0.13815864 | 1.4804062 | -0.317186583 | -0.70301863 |  0.960619367 | -0.35390149 |\n| that |  0.29461876 | -0.87759121 | 0.05826057 |  0.37605319 | -1.05454057 |  0.35491667 |  0.357027236 | -0.0322892948 | -0.8261352 | -0.416421353 | ⋯ | 0.8274029 |  0.20535042 | -0.163242226 |  0.43860085 | -0.51347366 | 1.6675639 | -0.123437992 | -0.38485505 |  0.190445567 |  0.02113333 |\n| are | -1.01009041 | -0.38996589 | 0.78437092 | -1.20336579 |  0.32266309 |  1.77786924 | -0.198341769 |  0.9877242543 | -0.9196615 |  0.262582247 | ⋯ | 0.8245676 |  0.72205429 | -0.611866868 |  2.17995070 | -0.43209494 | 0.7141540 | -0.310214277 | -0.62570673 |  0.790375902 | -0.38930963 |\n| me |  0.91295703 | -0.67142812 | 0.89832726 |  0.72298262 | -0.33188895 |  0.18575597 |  0.095386702 |  0.3446390058 | -0.5322870 | -0.051038622 | ⋯ | 0.9874224 |  0.59983195 |  0.634230040 |  0.82089915 | -1.02258828 | 1.7741451 |  0.354590480 |  0.63372518 |  0.275596054 |  0.36610939 |\n| of |  0.02042433 | -0.61814765 | 0.41187393 |  0.78508502 | -0.62855862 | -0.17142417 | -0.196826144 |  1.0146473032 | -0.5444330 |  0.653661660 | ⋯ | 0.7545896 |  0.11419678 |  0.319663670 |  1.11015049 |  0.70160431 | 0.6867089 | -0.244857004 | -0.37555101 | -0.183021007 | -1.45380483 |\n| in |  0.29488392 | -0.90998782 | 0.58842261 |  0.49775559 | -0.56231897 | -0.29213216 | -0.099918729 |  0.8558565963 | -0.9014696 | -0.058092618 | ⋯ | 0.7614794 |  0.93653439 | -0.324843002 |  0.67796794 |  0.43627938 | 0.7676684 | -1.628720396 | -0.29710615 |  0.173605579 | -1.31176620 |\n| it |  0.88038745 | -0.27740931 | 0.33769112 |  0.83809891 | -1.06788656 |  0.65374539 | -0.413026174 |  0.1071945725 | -0.7656125 | -0.031481317 | ⋯ | 0.9401578 | -0.01717349 | -0.185548671 |  0.40555803 |  0.26815605 | 1.6548084 | -0.330136184 | -0.38485989 |  0.355766796 | -0.56673340 |\n| you |  0.02921793 | -0.69956947 | 0.65619414 |  0.19990306 | -0.55782712 |  0.23775012 |  0.557279978 |  0.2540646605 | -0.6562819 | -0.102652868 | ⋯ | 0.9707404 |  1.01878699 |  0.051109162 |  0.56709298 | -1.09163977 | 1.8576337 |  0.325543294 | -0.27147131 |  0.545372987 | -0.13519577 |\n| my | -0.36326315 |  0.13292878 | 1.43764062 |  0.96306913 | -0.14513789 |  0.09596304 | -0.681646435 |  0.4329776124 | -0.7721462 | -1.049417196 | ⋯ | 1.9199913 | -0.25758989 | -0.724352957 |  0.93463658 | -0.11169862 | 1.5346297 | -0.788939348 | -0.27372689 | -0.436236316 |  0.39523997 |\n| is | -0.26110188 | -0.10388824 | 0.54050263 |  0.82070089 | -0.47990125 |  0.10403162 | -0.047889726 |  0.3543069185 | -1.4300532 |  0.220242903 | ⋯ | 1.4188999 |  0.05045011 |  0.181550703 |  0.78262696 |  1.31974635 | 1.8564849 |  0.246831066 | -0.62006157 |  0.448739600 | -0.52222979 |\n| to |  0.04527890 | -0.34062380 | 0.17724608 | -0.03262842 | -1.08603362 | -0.69119638 |  0.153098458 |  0.3512644683 | -0.6422789 | -0.708777481 | ⋯ | 0.9725228 |  0.49056372 |  0.078905810 |  0.31924556 | -0.67267930 | 1.7620503 | -0.505294399 | -0.34906607 |  0.126604838 | -0.54660212 |\n| and |  0.49708222 | -0.05538622 | 0.64361148 | -0.17765136 | -0.27773266 |  0.02712476 | -0.705487112 |  0.6719607646 | -0.7141925 |  0.155614203 | ⋯ | 1.1783046 | -0.18493637 | -0.238077333 |  0.81325745 | -0.25105040 | 0.9476438 | -0.762718670 | -0.34686658 |  0.076044470 | -0.30018538 |\n| a |  0.03933321 |  0.36935742 | 0.47268360 |  1.44602852 | -0.31805893 | -1.72934639 |  0.614659565 |  0.2250980756 | -1.3363209 |  0.244922024 | ⋯ | 1.1248268 |  0.02815606 | -0.455767982 | -0.91003319 | -0.21015546 | 1.3258001 |  0.041061325 | -1.26043371 |  0.094672523 | -0.58245419 |\n| the |  0.29750320 | -0.58607189 | 0.22902895 |  0.34357388 | -1.07724707 | -0.44501136 |  0.049346221 |  0.7785464956 | -0.6021630 |  0.141874414 | ⋯ | 0.7296454 |  0.98887298 | -0.277144371 |  0.13211323 |  0.45814443 | 0.9946228 | -1.611777582 |  0.13637256 |  0.509871545 | -1.10640435 |\n| i |  0.47940454 | -0.16366432 | 0.70160325 |  0.79155248 | -0.82320585 | -0.33732812 |  0.736717288 | -0.1692837726 | -0.5418732 | -0.138433952 | ⋯ | 1.1956301 |  0.88732448 | -0.492154390 |  0.44235429 | -0.87338609 | 1.5814970 | -0.295314993 | -0.63478018 |  0.355809566 | -0.27658641 |\n\n",
            "text/latex": "A matrix: 14267 × 50 of type dbl\n\\begin{tabular}{r|lllllllllllllllllllll}\n\t1837 & -0.39605574 & -0.02061349 & -0.292531632 & -0.49664328 &  0.40783190 &  0.851044482 & -0.39696587 & -0.27849286 &  0.27692855 &  0.570315492 & ⋯ &  0.02241029 & -0.57090033 &  0.28269541 & -0.709638090 & -0.086413878 & -0.436961662 &  0.145116474 & -0.13954358 &  0.25301617 &  0.22173371\\\\\n\t1841 & -0.78071031 &  0.65679353 & -0.050169592 & -0.44028214 & -0.20699434 &  0.545783287 &  0.39728389 &  0.25466314 & -0.30259115 &  0.170385098 & ⋯ & -0.50191797 & -0.35069074 & -0.73582634 &  0.092209004 &  0.257043205 & -0.581941567 &  0.130368074 & -0.01831186 & -0.25701126 & -0.47245659\\\\\n\t1881 & -0.04010316 & -0.10134305 &  0.174045113 & -0.56769329 &  0.38104333 &  0.343796140 & -0.32884733 & -0.19615921 &  0.53750568 &  0.134096555 & ⋯ & -0.02316887 & -0.58453249 & -0.27100670 & -0.472853220 &  0.516830218 & -0.339338338 &  0.411892514 &  0.97466772 & -0.20560002 &  0.68405675\\\\\n\t2005 &  0.15787077 &  0.71031086 & -0.313078482 & -0.61102057 &  0.06495423 &  0.371926194 & -0.26801623 & -0.86445663 &  0.01352563 & -0.171241807 & ⋯ & -0.82696919 &  0.16742684 &  0.94363227 & -0.590631081 &  0.701651729 &  0.077559228 &  0.320037468 &  0.10129664 & -0.45100058 &  0.20574856\\\\\n\t36 &  0.19290715 & -0.52808743 & -0.423933116 & -0.39152847 &  0.43662860 &  0.631995358 &  0.09486522 & -0.61107417 &  0.50077352 &  0.449836962 & ⋯ & -0.33239024 & -0.03064664 & -0.48434318 &  0.302711704 & -0.035767930 & -0.252195130 &  0.004768143 &  0.22295622 & -0.16733290 & -0.24080546\\\\\n\t38 & -0.06613049 &  0.59058818 &  0.032173097 &  0.44651101 &  0.07999208 & -0.254271109 & -0.14656225 & -0.30944246 & -0.39287467 & -0.108862124 & ⋯ & -0.11765821 & -0.75640020 & -0.13223140 & -0.325570517 & -0.068856731 & -0.689340349 & -0.160185505 &  0.34359886 & -0.21039644 &  0.01838548\\\\\n\t39 & -0.13287689 &  0.28692471 & -0.003638418 & -0.69530049 &  0.43808501 &  0.357472266 &  0.25758576 & -1.00892284 & -0.01337265 & -0.591372447 & ⋯ & -0.53878903 & -0.18590096 &  0.24206018 &  0.622937921 & -0.183574815 &  0.001330743 & -0.091965269 &  0.49289726 & -0.26660598 & -0.24322074\\\\\n\t52 & -0.19872705 & -0.20982830 &  0.326931644 & -0.49967092 &  0.07001185 & -0.105890798 & -0.32086134 &  0.13573098 &  0.83251341 &  0.306377047 & ⋯ & -0.33254810 &  0.12059476 &  0.67515880 &  0.240465878 &  0.191630896 & -0.099129157 &  0.340868096 & -0.12504141 &  0.46920537 &  0.53592542\\\\\n\t5â &  0.16738605 &  0.43949076 & -0.655501132 &  0.07723001 & -0.09199876 & -0.034267178 &  0.04048797 & -0.17018697 &  0.52170505 &  0.603731582 & ⋯ & -0.85178506 & -0.30215470 & -1.08300433 & -0.460445616 &  0.370161831 & -0.519443597 & -0.328239128 &  0.69915124 & -0.99653031 & -0.42381830\\\\\n\t600 & -0.18319427 &  0.10173004 &  0.267230059 & -0.35690795 &  0.43876954 & -0.464610861 &  0.49667306 & -0.13993497 &  0.41457057 &  0.333022652 & ⋯ & -0.49316066 & -0.36144923 &  0.25198538 & -0.132789275 & -0.021780580 & -1.235329792 &  0.112048058 & -0.42602652 &  0.31026247 &  0.24424602\\\\\n\tabcdefg &  0.70735335 &  0.03986606 & -0.761714992 & -0.80056051 &  0.53046658 &  0.813294633 &  0.03168502 & -0.83020056 &  0.54576263 &  0.382438103 & ⋯ &  0.14297748 &  0.58938418 &  0.43381397 & -0.266562818 &  0.073584107 &  0.101853044 &  0.715129888 &  0.35189552 &  0.17785692 & -0.61088021\\\\\n\tabroad & -0.08418838 &  0.65628976 & -0.406291943 & -0.58331686 &  0.48694521 &  0.090336337 &  0.55424315 & -0.08964373 &  0.08265955 & -0.715753155 & ⋯ &  0.19349736 &  0.51979299 &  0.60822378 & -0.556848371 & -0.139043952 & -0.181291751 &  0.589252357 &  0.68893515 & -0.29230629 &  0.60544112\\\\\n\tabruptly & -0.75465511 & -0.38653390 & -0.569676230 &  0.25027457 &  0.18802824 &  0.063757877 & -0.18784266 & -0.68724042 &  0.81150829 & -0.015416972 & ⋯ & -0.29018679 &  0.37914324 & -0.56447501 & -0.505668540 &  0.335404427 &  0.097786019 & -0.388914190 & -0.38557285 &  0.10788126 &  1.07054095\\\\\n\tabsorbing & -0.50020257 & -0.33860186 & -0.777045595 & -0.17992889 & -0.58877689 &  0.543358817 & -0.33775998 &  0.53389019 & -0.38384163 & -0.005132127 & ⋯ & -0.42977623 &  0.10758341 & -0.26810107 & -0.287205026 & -0.312171091 & -0.357957836 &  0.167526479 &  0.42167415 & -0.15069332 &  0.27986481\\\\\n\taccomplishments & -0.41715199 &  0.11152296 &  0.467634402 &  0.32239662 &  0.64448575 &  0.289161685 & -0.59506542 &  0.07581241 &  0.25354535 & -0.042860936 & ⋯ & -0.07269870 & -0.48796473 &  0.42911527 &  0.296464644 &  0.184565544 &  0.242916970 & -0.297316351 & -0.02192918 &  0.02381857 &  1.01208497\\\\\n\taccused & -0.26991649 &  0.03288589 & -0.169466644 &  0.52264325 &  0.47107478 &  0.354373016 &  0.81422378 & -0.41457847 &  0.48387971 &  0.522368365 & ⋯ &  0.27525556 & -0.73370105 &  0.46510937 &  0.603896149 &  0.979517578 & -0.196961453 & -0.328864534 & -0.58027338 & -0.52653290 &  0.45906009\\\\\n\tachievement &  0.39997079 &  0.41026612 & -0.341842703 & -0.41467966 & -0.31928564 & -0.005160941 &  0.36743120 &  0.45298109 &  0.70166833 & -0.574403694 & ⋯ & -0.47364188 & -0.88646287 &  0.26261689 & -0.005924459 &  0.660246560 & -0.723082428 & -0.149632690 & -0.13746019 & -0.06720847 &  0.22176334\\\\\n\tacquired &  0.48017198 &  0.19959144 &  0.017158356 & -0.21133543 &  0.37107769 & -0.148840688 & -0.37196266 & -0.13091843 &  0.22530481 & -0.029761000 & ⋯ & -0.19462462 & -0.03255923 &  0.38051812 & -0.439290708 &  0.152316245 & -0.474707575 & -0.062887407 &  0.03274934 & -0.24725423 &  0.24409540\\\\\n\tacres &  0.02621845 &  0.20148900 & -0.750877582 &  0.23827775 & -0.22556586 & -0.082994703 & -0.24850354 & -0.02573336 & -0.39737033 & -0.686905678 & ⋯ & -0.69901139 & -0.45011647 &  0.10235671 & -0.256821371 &  0.664099741 &  0.234280279 &  0.411153731 &  0.39836951 &  0.05258725 & -0.32525395\\\\\n\tadams &  0.26623502 &  1.01935488 & -0.402454807 & -0.22125943 &  0.04596833 &  0.067207560 &  0.01206120 & -0.35462093 &  0.43541320 & -0.379243165 & ⋯ & -0.15590756 & -0.57444189 &  0.39395363 &  0.366176218 & -0.230617141 & -0.443790956 & -0.056869235 &  0.19434052 &  0.12382523 &  0.04268834\\\\\n\taddress & -0.10785436 & -0.72239178 & -0.014587346 & -0.20541192 &  0.06330883 &  0.296418469 & -0.01966000 & -0.23844020 &  0.60724074 &  0.023561460 & ⋯ &  0.08453997 & -0.17504163 &  0.79019792 & -0.299842167 &  0.005570082 & -0.697858051 &  0.967882308 &  0.74267883 & -0.23478951 & -0.08638539\\\\\n\tadjust & -0.24831227 &  0.30466502 & -0.031096349 & -0.04317058 & -0.75828243 &  0.366924123 &  0.87349832 & -0.68680689 &  0.54500601 & -0.343893322 & ⋯ &  0.37090373 & -0.01961698 & -0.10056845 & -0.161753172 & -0.110804855 & -0.080501837 & -0.833086287 &  0.18999399 & -0.60048538 &  0.52134227\\\\\n\tadmires & -0.31729958 &  0.44147387 & -0.021398545 &  0.12188454 &  0.71702030 & -0.419285741 &  0.24584987 &  0.22101916 &  0.73687478 & -0.558588185 & ⋯ &  0.21235213 & -0.07358972 & -0.18027356 &  0.750364485 & -0.452595970 & -0.414619174 & -0.129256578 & -0.12132017 & -0.75372962 &  0.85117923\\\\\n\tadmits & -0.95281973 &  0.12204455 & -0.205021076 &  0.17699643 & -0.02412552 &  0.549661586 & -0.12190072 & -0.29200957 &  0.62044908 & -0.391099078 & ⋯ & -0.34380550 & -0.32053749 &  0.23215307 & -0.496023417 & -0.126543225 & -0.613394837 &  0.450257382 &  0.47289568 &  0.25420994 & -0.19119665\\\\\n\tadmitted &  0.18759031 & -0.32369531 & -0.938430479 & -0.17741197 & -0.47941412 &  0.018157922 &  0.32523834 &  0.09749735 &  0.86611433 & -0.912179120 & ⋯ & -0.56393874 & -0.57302486 &  0.03829672 & -0.304267374 &  0.291767974 & -0.205611595 &  0.173763380 & -0.42971896 & -0.54863718 &  0.64700551\\\\\n\tadorn & -0.34048457 & -0.53606391 & -0.166729348 & -0.04248947 & -0.18723637 & -0.421273007 &  0.47392090 & -0.12530631 & -0.04284201 &  0.243934259 & ⋯ & -0.31114129 & -0.17483361 &  0.24586344 &  0.737974531 & -0.031574930 & -0.756253249 &  0.190669169 & -0.43936459 & -0.23082647 &  0.11158748\\\\\n\tadrenalin & -0.20606141 & -0.06482557 &  0.295521636 & -0.42895470 &  0.03211840 &  0.931785550 & -0.55653735 & -0.22255087 &  0.62056343 & -0.028575716 & ⋯ & -0.12886131 & -0.06061505 & -0.67201540 & -0.237778428 &  0.292461247 & -0.500908995 & -0.029012521 &  0.30806042 &  0.25517963 &  0.66449649\\\\\n\tadress &  0.78366920 & -0.66037199 & -0.794603296 &  0.15212204 & -0.06620859 & -0.015640330 & -0.10340023 & -0.34022111 & -0.14894531 & -0.339456034 & ⋯ & -0.54330462 & -0.64177143 &  0.21696370 & -0.269471441 &  0.175601141 & -0.146203963 &  0.245779959 & -0.07227204 & -0.52550125 &  0.23499252\\\\\n\tadrift &  0.05843164 & -0.58770716 & -0.536694405 & -0.65818455 &  0.11936475 & -0.065375977 & -0.13477620 &  0.09075554 & -0.05782813 & -0.445612478 & ⋯ & -0.61756805 & -0.16649287 & -0.25527553 &  0.555799185 &  0.612421355 & -0.834533678 & -0.337302176 &  0.22362290 & -0.09379581 &  0.11036846\\\\\n\tadvanced &  0.56084521 &  1.14434186 & -0.385504901 & -0.38727650 &  0.20843386 &  0.026028555 &  0.63470848 & -0.02157528 &  0.64902722 & -0.129253175 & ⋯ & -0.10880402 & -0.33665753 &  0.16229289 & -0.361123811 & -0.306083825 &  0.121554167 &  0.802122076 &  0.53786073 & -0.87075226 & -0.35036325\\\\\n\t⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋱ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n\tyour & -0.60564646 & -0.62553236 & 1.24490314 &  0.07566892 &  0.06613355 &  0.20188949 & -0.409003665 &  0.5676421848 & -0.8230869 & -0.699537680 & ⋯ & 1.2401653 &  0.20952914 & -0.893507520 &  1.10687310 & -0.02433232 & 1.1524797 & -0.511380807 & -0.31819395 &  0.021489723 &  0.26303308\\\\\n\tshe &  1.25638190 & -0.27417210 & 0.78292581 &  0.64209586 & -0.64827656 &  0.47055208 & -0.908629744 & -0.4771845198 & -0.5481247 & -0.671014686 & ⋯ & 1.5116232 & -0.63008666 &  0.288729925 & -0.09693033 & -0.34142356 & 1.4555342 & -0.502895692 &  0.11742506 & -0.016573978 &  0.85044696\\\\\n\tlove & -0.47156184 & -0.41615846 & 0.58087619 &  0.16058354 & -0.43777360 &  0.57852725 &  0.479625783 &  0.7178994202 & -0.7162056 &  0.181676720 & ⋯ & 1.3686970 &  0.35205021 &  0.242322126 &  1.10917708 & -0.83019695 & 1.6898825 &  0.526807611 & -0.03894946 &  0.456800124 & -0.89155241\\\\\n\tthey & -0.35827850 & -0.30550349 & 0.41978209 & -0.90778265 & -0.43329577 &  0.49337017 & -0.403912329 &  0.2118026287 & -0.3351952 & -0.241848583 & ⋯ & 0.7515785 &  0.56610648 & -0.252167427 &  0.65779104 & -1.27628312 & 1.0787799 & -0.648060347 & -0.79852493 &  0.551091693 & -0.14490979\\\\\n\tall &  0.11720411 & -0.74526340 & 0.20247411 &  0.28941858 & -0.92638975 &  0.56686602 & -0.172279828 &  0.7941381581 & -0.3109786 & -0.614249410 & ⋯ & 0.6227535 &  0.30527561 & -0.126590147 &  1.30020891 & -0.08803566 & 0.8815118 & -0.885388090 & -0.67187065 &  0.524857500 & -0.40063246\\\\\n\tas & -0.11624688 & -0.25615083 & 0.63723694 & -0.15859595 &  0.26664789 &  0.03729823 & -0.714140545 &  0.3447922780 & -1.2119347 &  1.045180778 & ⋯ & 0.5350480 & -0.09392316 & -0.840557636 &  0.12374327 & -0.20070733 & 1.3998739 & -0.623881007 & -0.67044334 &  0.337694542 &  0.11456077\\\\\n\tfor & -0.25557120 & -1.22774622 & 0.36732031 &  1.15379730 & -0.81105382 & -0.58278982 & -0.007714816 &  0.1867806799 & -0.3772156 & -0.327810962 & ⋯ & 0.8121711 &  0.90376398 &  0.223292297 &  0.76098587 &  0.20402389 & 1.0796115 & -0.136541600 & -0.84632118 & -0.184942038 & -0.51611075\\\\\n\tbe & -0.14850891 & -0.98468027 & 0.08846009 &  0.24884741 &  0.13636602 &  0.13814258 &  0.221039144 & -0.3272235393 & -1.0083243 & -0.143189207 & ⋯ & 0.7192127 &  0.65354656 & -0.163325842 &  0.87071768 & -0.36572406 & 1.5257384 &  0.333888251 & -0.39944828 &  0.806497218 &  0.14134930\\\\\n\twhen &  0.57533352 & -0.53009848 & 1.01794899 &  0.72600962 & -0.68010104 &  0.21665274 & -0.319655988 &  0.0009623705 & -0.5837773 &  0.008194372 & ⋯ & 1.1439938 &  0.33273533 &  0.007094395 &  0.53755694 & -0.34664721 & 1.2436099 & -0.448986056 & -0.60906069 &  0.226135761 &  0.11392773\\\\\n\twith &  0.17133929 & -0.17838819 & 0.58251350 &  0.11253254 & -0.38398995 & -0.51119930 & -0.053675252 &  0.6514033853 & -0.4789235 &  0.501062456 & ⋯ & 1.1618845 & -0.22637164 & -0.363779317 &  1.00956877 & -0.65822579 & 0.6530590 & -1.009942032 & -0.70892681 &  0.265583915 & -0.69986409\\\\\n\tso &  0.05879581 & -0.53859237 & 0.43241685 & -0.19798659 &  0.37991846 &  0.66512085 & -0.356972696 &  0.0439883445 & -0.6670437 & -0.047874663 & ⋯ & 1.0497711 &  0.42287794 & -0.634407802 &  0.66041415 & -0.25366041 & 1.9301640 & -0.401711816 & -0.60208501 &  0.260621312 &  0.06687380\\\\\n\ton &  0.45676682 &  0.14578002 & 0.71469450 &  0.69460988 & -0.55329123 & -0.83893077 & -0.333487314 &  0.0208520019 & -1.1300300 & -1.062147051 & ⋯ & 1.0803135 &  0.78212691 & -1.114261244 &  0.68217380 & -0.01680861 & 0.7734562 & -1.472068964 & -0.97036547 &  0.680163351 & -0.55308657\\\\\n\twas &  1.26103268 & -0.87521985 & 0.42591786 &  1.44752023 & -0.56292737 & -0.53824005 &  0.700908841 & -1.0281724581 & -0.7886144 & -0.963085681 & ⋯ & 1.0668965 &  0.17199371 & -0.151018057 &  0.33248939 &  0.01220992 & 1.2473095 & -0.494631285 & -0.82375822 &  0.323448808 &  0.44993179\\\\\n\tbut &  0.07047070 & -0.42080162 & 0.25806188 &  0.40316479 & -0.59542069 &  0.28235011 & -0.349787195 &  0.1637144872 & -0.8339218 & -0.355345842 & ⋯ & 0.9462293 &  0.24685036 &  0.070578389 &  0.55861198 & -0.22122495 & 1.4829625 &  0.004946889 & -0.48550488 &  0.359658026 &  0.21088970\\\\\n\the &  1.25229438 &  0.12016153 & 0.63392055 &  0.84683830 & -0.90478847 & -0.35824703 & -0.677012685 & -0.7382110010 & -0.6369223 & -0.589857265 & ⋯ & 1.5686786 & -0.46246393 &  0.553926860 & -0.25155399 & -0.22799796 & 1.4709812 & -0.989057028 & -0.09478956 &  0.002057899 &  1.02377057\\\\\n\tlike & -0.04622273 &  0.29959450 & 0.92314664 & -0.26548874 & -0.47171732 &  0.58351138 & -0.604306667 &  0.3800892986 & -1.1940816 &  0.825574944 & ⋯ & 0.8058586 & -0.23305871 & -0.226500325 &  0.56541311 & -0.13815864 & 1.4804062 & -0.317186583 & -0.70301863 &  0.960619367 & -0.35390149\\\\\n\tthat &  0.29461876 & -0.87759121 & 0.05826057 &  0.37605319 & -1.05454057 &  0.35491667 &  0.357027236 & -0.0322892948 & -0.8261352 & -0.416421353 & ⋯ & 0.8274029 &  0.20535042 & -0.163242226 &  0.43860085 & -0.51347366 & 1.6675639 & -0.123437992 & -0.38485505 &  0.190445567 &  0.02113333\\\\\n\tare & -1.01009041 & -0.38996589 & 0.78437092 & -1.20336579 &  0.32266309 &  1.77786924 & -0.198341769 &  0.9877242543 & -0.9196615 &  0.262582247 & ⋯ & 0.8245676 &  0.72205429 & -0.611866868 &  2.17995070 & -0.43209494 & 0.7141540 & -0.310214277 & -0.62570673 &  0.790375902 & -0.38930963\\\\\n\tme &  0.91295703 & -0.67142812 & 0.89832726 &  0.72298262 & -0.33188895 &  0.18575597 &  0.095386702 &  0.3446390058 & -0.5322870 & -0.051038622 & ⋯ & 0.9874224 &  0.59983195 &  0.634230040 &  0.82089915 & -1.02258828 & 1.7741451 &  0.354590480 &  0.63372518 &  0.275596054 &  0.36610939\\\\\n\tof &  0.02042433 & -0.61814765 & 0.41187393 &  0.78508502 & -0.62855862 & -0.17142417 & -0.196826144 &  1.0146473032 & -0.5444330 &  0.653661660 & ⋯ & 0.7545896 &  0.11419678 &  0.319663670 &  1.11015049 &  0.70160431 & 0.6867089 & -0.244857004 & -0.37555101 & -0.183021007 & -1.45380483\\\\\n\tin &  0.29488392 & -0.90998782 & 0.58842261 &  0.49775559 & -0.56231897 & -0.29213216 & -0.099918729 &  0.8558565963 & -0.9014696 & -0.058092618 & ⋯ & 0.7614794 &  0.93653439 & -0.324843002 &  0.67796794 &  0.43627938 & 0.7676684 & -1.628720396 & -0.29710615 &  0.173605579 & -1.31176620\\\\\n\tit &  0.88038745 & -0.27740931 & 0.33769112 &  0.83809891 & -1.06788656 &  0.65374539 & -0.413026174 &  0.1071945725 & -0.7656125 & -0.031481317 & ⋯ & 0.9401578 & -0.01717349 & -0.185548671 &  0.40555803 &  0.26815605 & 1.6548084 & -0.330136184 & -0.38485989 &  0.355766796 & -0.56673340\\\\\n\tyou &  0.02921793 & -0.69956947 & 0.65619414 &  0.19990306 & -0.55782712 &  0.23775012 &  0.557279978 &  0.2540646605 & -0.6562819 & -0.102652868 & ⋯ & 0.9707404 &  1.01878699 &  0.051109162 &  0.56709298 & -1.09163977 & 1.8576337 &  0.325543294 & -0.27147131 &  0.545372987 & -0.13519577\\\\\n\tmy & -0.36326315 &  0.13292878 & 1.43764062 &  0.96306913 & -0.14513789 &  0.09596304 & -0.681646435 &  0.4329776124 & -0.7721462 & -1.049417196 & ⋯ & 1.9199913 & -0.25758989 & -0.724352957 &  0.93463658 & -0.11169862 & 1.5346297 & -0.788939348 & -0.27372689 & -0.436236316 &  0.39523997\\\\\n\tis & -0.26110188 & -0.10388824 & 0.54050263 &  0.82070089 & -0.47990125 &  0.10403162 & -0.047889726 &  0.3543069185 & -1.4300532 &  0.220242903 & ⋯ & 1.4188999 &  0.05045011 &  0.181550703 &  0.78262696 &  1.31974635 & 1.8564849 &  0.246831066 & -0.62006157 &  0.448739600 & -0.52222979\\\\\n\tto &  0.04527890 & -0.34062380 & 0.17724608 & -0.03262842 & -1.08603362 & -0.69119638 &  0.153098458 &  0.3512644683 & -0.6422789 & -0.708777481 & ⋯ & 0.9725228 &  0.49056372 &  0.078905810 &  0.31924556 & -0.67267930 & 1.7620503 & -0.505294399 & -0.34906607 &  0.126604838 & -0.54660212\\\\\n\tand &  0.49708222 & -0.05538622 & 0.64361148 & -0.17765136 & -0.27773266 &  0.02712476 & -0.705487112 &  0.6719607646 & -0.7141925 &  0.155614203 & ⋯ & 1.1783046 & -0.18493637 & -0.238077333 &  0.81325745 & -0.25105040 & 0.9476438 & -0.762718670 & -0.34686658 &  0.076044470 & -0.30018538\\\\\n\ta &  0.03933321 &  0.36935742 & 0.47268360 &  1.44602852 & -0.31805893 & -1.72934639 &  0.614659565 &  0.2250980756 & -1.3363209 &  0.244922024 & ⋯ & 1.1248268 &  0.02815606 & -0.455767982 & -0.91003319 & -0.21015546 & 1.3258001 &  0.041061325 & -1.26043371 &  0.094672523 & -0.58245419\\\\\n\tthe &  0.29750320 & -0.58607189 & 0.22902895 &  0.34357388 & -1.07724707 & -0.44501136 &  0.049346221 &  0.7785464956 & -0.6021630 &  0.141874414 & ⋯ & 0.7296454 &  0.98887298 & -0.277144371 &  0.13211323 &  0.45814443 & 0.9946228 & -1.611777582 &  0.13637256 &  0.509871545 & -1.10640435\\\\\n\ti &  0.47940454 & -0.16366432 & 0.70160325 &  0.79155248 & -0.82320585 & -0.33732812 &  0.736717288 & -0.1692837726 & -0.5418732 & -0.138433952 & ⋯ & 1.1956301 &  0.88732448 & -0.492154390 &  0.44235429 & -0.87338609 & 1.5814970 & -0.295314993 & -0.63478018 &  0.355809566 & -0.27658641\\\\\n\\end{tabular}\n",
            "text/plain": [
              "                [,1]        [,2]        [,3]         [,4]        [,5]       \n",
              "1837            -0.39605574 -0.02061349 -0.292531632 -0.49664328  0.40783190\n",
              "1841            -0.78071031  0.65679353 -0.050169592 -0.44028214 -0.20699434\n",
              "1881            -0.04010316 -0.10134305  0.174045113 -0.56769329  0.38104333\n",
              "2005             0.15787077  0.71031086 -0.313078482 -0.61102057  0.06495423\n",
              "36               0.19290715 -0.52808743 -0.423933116 -0.39152847  0.43662860\n",
              "38              -0.06613049  0.59058818  0.032173097  0.44651101  0.07999208\n",
              "39              -0.13287689  0.28692471 -0.003638418 -0.69530049  0.43808501\n",
              "52              -0.19872705 -0.20982830  0.326931644 -0.49967092  0.07001185\n",
              "5â               0.16738605  0.43949076 -0.655501132  0.07723001 -0.09199876\n",
              "600             -0.18319427  0.10173004  0.267230059 -0.35690795  0.43876954\n",
              "abcdefg          0.70735335  0.03986606 -0.761714992 -0.80056051  0.53046658\n",
              "abroad          -0.08418838  0.65628976 -0.406291943 -0.58331686  0.48694521\n",
              "abruptly        -0.75465511 -0.38653390 -0.569676230  0.25027457  0.18802824\n",
              "absorbing       -0.50020257 -0.33860186 -0.777045595 -0.17992889 -0.58877689\n",
              "accomplishments -0.41715199  0.11152296  0.467634402  0.32239662  0.64448575\n",
              "accused         -0.26991649  0.03288589 -0.169466644  0.52264325  0.47107478\n",
              "achievement      0.39997079  0.41026612 -0.341842703 -0.41467966 -0.31928564\n",
              "acquired         0.48017198  0.19959144  0.017158356 -0.21133543  0.37107769\n",
              "acres            0.02621845  0.20148900 -0.750877582  0.23827775 -0.22556586\n",
              "adams            0.26623502  1.01935488 -0.402454807 -0.22125943  0.04596833\n",
              "address         -0.10785436 -0.72239178 -0.014587346 -0.20541192  0.06330883\n",
              "adjust          -0.24831227  0.30466502 -0.031096349 -0.04317058 -0.75828243\n",
              "admires         -0.31729958  0.44147387 -0.021398545  0.12188454  0.71702030\n",
              "admits          -0.95281973  0.12204455 -0.205021076  0.17699643 -0.02412552\n",
              "admitted         0.18759031 -0.32369531 -0.938430479 -0.17741197 -0.47941412\n",
              "adorn           -0.34048457 -0.53606391 -0.166729348 -0.04248947 -0.18723637\n",
              "adrenalin       -0.20606141 -0.06482557  0.295521636 -0.42895470  0.03211840\n",
              "adress           0.78366920 -0.66037199 -0.794603296  0.15212204 -0.06620859\n",
              "adrift           0.05843164 -0.58770716 -0.536694405 -0.65818455  0.11936475\n",
              "advanced         0.56084521  1.14434186 -0.385504901 -0.38727650  0.20843386\n",
              "⋮               ⋮           ⋮           ⋮            ⋮           ⋮          \n",
              "your            -0.60564646 -0.62553236 1.24490314    0.07566892  0.06613355\n",
              "she              1.25638190 -0.27417210 0.78292581    0.64209586 -0.64827656\n",
              "love            -0.47156184 -0.41615846 0.58087619    0.16058354 -0.43777360\n",
              "they            -0.35827850 -0.30550349 0.41978209   -0.90778265 -0.43329577\n",
              "all              0.11720411 -0.74526340 0.20247411    0.28941858 -0.92638975\n",
              "as              -0.11624688 -0.25615083 0.63723694   -0.15859595  0.26664789\n",
              "for             -0.25557120 -1.22774622 0.36732031    1.15379730 -0.81105382\n",
              "be              -0.14850891 -0.98468027 0.08846009    0.24884741  0.13636602\n",
              "when             0.57533352 -0.53009848 1.01794899    0.72600962 -0.68010104\n",
              "with             0.17133929 -0.17838819 0.58251350    0.11253254 -0.38398995\n",
              "so               0.05879581 -0.53859237 0.43241685   -0.19798659  0.37991846\n",
              "on               0.45676682  0.14578002 0.71469450    0.69460988 -0.55329123\n",
              "was              1.26103268 -0.87521985 0.42591786    1.44752023 -0.56292737\n",
              "but              0.07047070 -0.42080162 0.25806188    0.40316479 -0.59542069\n",
              "he               1.25229438  0.12016153 0.63392055    0.84683830 -0.90478847\n",
              "like            -0.04622273  0.29959450 0.92314664   -0.26548874 -0.47171732\n",
              "that             0.29461876 -0.87759121 0.05826057    0.37605319 -1.05454057\n",
              "are             -1.01009041 -0.38996589 0.78437092   -1.20336579  0.32266309\n",
              "me               0.91295703 -0.67142812 0.89832726    0.72298262 -0.33188895\n",
              "of               0.02042433 -0.61814765 0.41187393    0.78508502 -0.62855862\n",
              "in               0.29488392 -0.90998782 0.58842261    0.49775559 -0.56231897\n",
              "it               0.88038745 -0.27740931 0.33769112    0.83809891 -1.06788656\n",
              "you              0.02921793 -0.69956947 0.65619414    0.19990306 -0.55782712\n",
              "my              -0.36326315  0.13292878 1.43764062    0.96306913 -0.14513789\n",
              "is              -0.26110188 -0.10388824 0.54050263    0.82070089 -0.47990125\n",
              "to               0.04527890 -0.34062380 0.17724608   -0.03262842 -1.08603362\n",
              "and              0.49708222 -0.05538622 0.64361148   -0.17765136 -0.27773266\n",
              "a                0.03933321  0.36935742 0.47268360    1.44602852 -0.31805893\n",
              "the              0.29750320 -0.58607189 0.22902895    0.34357388 -1.07724707\n",
              "i                0.47940454 -0.16366432 0.70160325    0.79155248 -0.82320585\n",
              "                [,6]         [,7]         [,8]          [,9]       \n",
              "1837             0.851044482 -0.39696587  -0.27849286    0.27692855\n",
              "1841             0.545783287  0.39728389   0.25466314   -0.30259115\n",
              "1881             0.343796140 -0.32884733  -0.19615921    0.53750568\n",
              "2005             0.371926194 -0.26801623  -0.86445663    0.01352563\n",
              "36               0.631995358  0.09486522  -0.61107417    0.50077352\n",
              "38              -0.254271109 -0.14656225  -0.30944246   -0.39287467\n",
              "39               0.357472266  0.25758576  -1.00892284   -0.01337265\n",
              "52              -0.105890798 -0.32086134   0.13573098    0.83251341\n",
              "5â              -0.034267178  0.04048797  -0.17018697    0.52170505\n",
              "600             -0.464610861  0.49667306  -0.13993497    0.41457057\n",
              "abcdefg          0.813294633  0.03168502  -0.83020056    0.54576263\n",
              "abroad           0.090336337  0.55424315  -0.08964373    0.08265955\n",
              "abruptly         0.063757877 -0.18784266  -0.68724042    0.81150829\n",
              "absorbing        0.543358817 -0.33775998   0.53389019   -0.38384163\n",
              "accomplishments  0.289161685 -0.59506542   0.07581241    0.25354535\n",
              "accused          0.354373016  0.81422378  -0.41457847    0.48387971\n",
              "achievement     -0.005160941  0.36743120   0.45298109    0.70166833\n",
              "acquired        -0.148840688 -0.37196266  -0.13091843    0.22530481\n",
              "acres           -0.082994703 -0.24850354  -0.02573336   -0.39737033\n",
              "adams            0.067207560  0.01206120  -0.35462093    0.43541320\n",
              "address          0.296418469 -0.01966000  -0.23844020    0.60724074\n",
              "adjust           0.366924123  0.87349832  -0.68680689    0.54500601\n",
              "admires         -0.419285741  0.24584987   0.22101916    0.73687478\n",
              "admits           0.549661586 -0.12190072  -0.29200957    0.62044908\n",
              "admitted         0.018157922  0.32523834   0.09749735    0.86611433\n",
              "adorn           -0.421273007  0.47392090  -0.12530631   -0.04284201\n",
              "adrenalin        0.931785550 -0.55653735  -0.22255087    0.62056343\n",
              "adress          -0.015640330 -0.10340023  -0.34022111   -0.14894531\n",
              "adrift          -0.065375977 -0.13477620   0.09075554   -0.05782813\n",
              "advanced         0.026028555  0.63470848  -0.02157528    0.64902722\n",
              "⋮               ⋮            ⋮            ⋮             ⋮          \n",
              "your             0.20188949  -0.409003665  0.5676421848 -0.8230869 \n",
              "she              0.47055208  -0.908629744 -0.4771845198 -0.5481247 \n",
              "love             0.57852725   0.479625783  0.7178994202 -0.7162056 \n",
              "they             0.49337017  -0.403912329  0.2118026287 -0.3351952 \n",
              "all              0.56686602  -0.172279828  0.7941381581 -0.3109786 \n",
              "as               0.03729823  -0.714140545  0.3447922780 -1.2119347 \n",
              "for             -0.58278982  -0.007714816  0.1867806799 -0.3772156 \n",
              "be               0.13814258   0.221039144 -0.3272235393 -1.0083243 \n",
              "when             0.21665274  -0.319655988  0.0009623705 -0.5837773 \n",
              "with            -0.51119930  -0.053675252  0.6514033853 -0.4789235 \n",
              "so               0.66512085  -0.356972696  0.0439883445 -0.6670437 \n",
              "on              -0.83893077  -0.333487314  0.0208520019 -1.1300300 \n",
              "was             -0.53824005   0.700908841 -1.0281724581 -0.7886144 \n",
              "but              0.28235011  -0.349787195  0.1637144872 -0.8339218 \n",
              "he              -0.35824703  -0.677012685 -0.7382110010 -0.6369223 \n",
              "like             0.58351138  -0.604306667  0.3800892986 -1.1940816 \n",
              "that             0.35491667   0.357027236 -0.0322892948 -0.8261352 \n",
              "are              1.77786924  -0.198341769  0.9877242543 -0.9196615 \n",
              "me               0.18575597   0.095386702  0.3446390058 -0.5322870 \n",
              "of              -0.17142417  -0.196826144  1.0146473032 -0.5444330 \n",
              "in              -0.29213216  -0.099918729  0.8558565963 -0.9014696 \n",
              "it               0.65374539  -0.413026174  0.1071945725 -0.7656125 \n",
              "you              0.23775012   0.557279978  0.2540646605 -0.6562819 \n",
              "my               0.09596304  -0.681646435  0.4329776124 -0.7721462 \n",
              "is               0.10403162  -0.047889726  0.3543069185 -1.4300532 \n",
              "to              -0.69119638   0.153098458  0.3512644683 -0.6422789 \n",
              "and              0.02712476  -0.705487112  0.6719607646 -0.7141925 \n",
              "a               -1.72934639   0.614659565  0.2250980756 -1.3363209 \n",
              "the             -0.44501136   0.049346221  0.7785464956 -0.6021630 \n",
              "i               -0.33732812   0.736717288 -0.1692837726 -0.5418732 \n",
              "                [,10]        [,11] [,12]       [,13]       [,14]       \n",
              "1837             0.570315492 ⋯      0.02241029 -0.57090033  0.28269541 \n",
              "1841             0.170385098 ⋯     -0.50191797 -0.35069074 -0.73582634 \n",
              "1881             0.134096555 ⋯     -0.02316887 -0.58453249 -0.27100670 \n",
              "2005            -0.171241807 ⋯     -0.82696919  0.16742684  0.94363227 \n",
              "36               0.449836962 ⋯     -0.33239024 -0.03064664 -0.48434318 \n",
              "38              -0.108862124 ⋯     -0.11765821 -0.75640020 -0.13223140 \n",
              "39              -0.591372447 ⋯     -0.53878903 -0.18590096  0.24206018 \n",
              "52               0.306377047 ⋯     -0.33254810  0.12059476  0.67515880 \n",
              "5â               0.603731582 ⋯     -0.85178506 -0.30215470 -1.08300433 \n",
              "600              0.333022652 ⋯     -0.49316066 -0.36144923  0.25198538 \n",
              "abcdefg          0.382438103 ⋯      0.14297748  0.58938418  0.43381397 \n",
              "abroad          -0.715753155 ⋯      0.19349736  0.51979299  0.60822378 \n",
              "abruptly        -0.015416972 ⋯     -0.29018679  0.37914324 -0.56447501 \n",
              "absorbing       -0.005132127 ⋯     -0.42977623  0.10758341 -0.26810107 \n",
              "accomplishments -0.042860936 ⋯     -0.07269870 -0.48796473  0.42911527 \n",
              "accused          0.522368365 ⋯      0.27525556 -0.73370105  0.46510937 \n",
              "achievement     -0.574403694 ⋯     -0.47364188 -0.88646287  0.26261689 \n",
              "acquired        -0.029761000 ⋯     -0.19462462 -0.03255923  0.38051812 \n",
              "acres           -0.686905678 ⋯     -0.69901139 -0.45011647  0.10235671 \n",
              "adams           -0.379243165 ⋯     -0.15590756 -0.57444189  0.39395363 \n",
              "address          0.023561460 ⋯      0.08453997 -0.17504163  0.79019792 \n",
              "adjust          -0.343893322 ⋯      0.37090373 -0.01961698 -0.10056845 \n",
              "admires         -0.558588185 ⋯      0.21235213 -0.07358972 -0.18027356 \n",
              "admits          -0.391099078 ⋯     -0.34380550 -0.32053749  0.23215307 \n",
              "admitted        -0.912179120 ⋯     -0.56393874 -0.57302486  0.03829672 \n",
              "adorn            0.243934259 ⋯     -0.31114129 -0.17483361  0.24586344 \n",
              "adrenalin       -0.028575716 ⋯     -0.12886131 -0.06061505 -0.67201540 \n",
              "adress          -0.339456034 ⋯     -0.54330462 -0.64177143  0.21696370 \n",
              "adrift          -0.445612478 ⋯     -0.61756805 -0.16649287 -0.25527553 \n",
              "advanced        -0.129253175 ⋯     -0.10880402 -0.33665753  0.16229289 \n",
              "⋮               ⋮            ⋱     ⋮           ⋮           ⋮           \n",
              "your            -0.699537680 ⋯     1.2401653    0.20952914 -0.893507520\n",
              "she             -0.671014686 ⋯     1.5116232   -0.63008666  0.288729925\n",
              "love             0.181676720 ⋯     1.3686970    0.35205021  0.242322126\n",
              "they            -0.241848583 ⋯     0.7515785    0.56610648 -0.252167427\n",
              "all             -0.614249410 ⋯     0.6227535    0.30527561 -0.126590147\n",
              "as               1.045180778 ⋯     0.5350480   -0.09392316 -0.840557636\n",
              "for             -0.327810962 ⋯     0.8121711    0.90376398  0.223292297\n",
              "be              -0.143189207 ⋯     0.7192127    0.65354656 -0.163325842\n",
              "when             0.008194372 ⋯     1.1439938    0.33273533  0.007094395\n",
              "with             0.501062456 ⋯     1.1618845   -0.22637164 -0.363779317\n",
              "so              -0.047874663 ⋯     1.0497711    0.42287794 -0.634407802\n",
              "on              -1.062147051 ⋯     1.0803135    0.78212691 -1.114261244\n",
              "was             -0.963085681 ⋯     1.0668965    0.17199371 -0.151018057\n",
              "but             -0.355345842 ⋯     0.9462293    0.24685036  0.070578389\n",
              "he              -0.589857265 ⋯     1.5686786   -0.46246393  0.553926860\n",
              "like             0.825574944 ⋯     0.8058586   -0.23305871 -0.226500325\n",
              "that            -0.416421353 ⋯     0.8274029    0.20535042 -0.163242226\n",
              "are              0.262582247 ⋯     0.8245676    0.72205429 -0.611866868\n",
              "me              -0.051038622 ⋯     0.9874224    0.59983195  0.634230040\n",
              "of               0.653661660 ⋯     0.7545896    0.11419678  0.319663670\n",
              "in              -0.058092618 ⋯     0.7614794    0.93653439 -0.324843002\n",
              "it              -0.031481317 ⋯     0.9401578   -0.01717349 -0.185548671\n",
              "you             -0.102652868 ⋯     0.9707404    1.01878699  0.051109162\n",
              "my              -1.049417196 ⋯     1.9199913   -0.25758989 -0.724352957\n",
              "is               0.220242903 ⋯     1.4188999    0.05045011  0.181550703\n",
              "to              -0.708777481 ⋯     0.9725228    0.49056372  0.078905810\n",
              "and              0.155614203 ⋯     1.1783046   -0.18493637 -0.238077333\n",
              "a                0.244922024 ⋯     1.1248268    0.02815606 -0.455767982\n",
              "the              0.141874414 ⋯     0.7296454    0.98887298 -0.277144371\n",
              "i               -0.138433952 ⋯     1.1956301    0.88732448 -0.492154390\n",
              "                [,15]        [,16]        [,17]        [,18]        [,19]      \n",
              "1837            -0.709638090 -0.086413878 -0.436961662  0.145116474 -0.13954358\n",
              "1841             0.092209004  0.257043205 -0.581941567  0.130368074 -0.01831186\n",
              "1881            -0.472853220  0.516830218 -0.339338338  0.411892514  0.97466772\n",
              "2005            -0.590631081  0.701651729  0.077559228  0.320037468  0.10129664\n",
              "36               0.302711704 -0.035767930 -0.252195130  0.004768143  0.22295622\n",
              "38              -0.325570517 -0.068856731 -0.689340349 -0.160185505  0.34359886\n",
              "39               0.622937921 -0.183574815  0.001330743 -0.091965269  0.49289726\n",
              "52               0.240465878  0.191630896 -0.099129157  0.340868096 -0.12504141\n",
              "5â              -0.460445616  0.370161831 -0.519443597 -0.328239128  0.69915124\n",
              "600             -0.132789275 -0.021780580 -1.235329792  0.112048058 -0.42602652\n",
              "abcdefg         -0.266562818  0.073584107  0.101853044  0.715129888  0.35189552\n",
              "abroad          -0.556848371 -0.139043952 -0.181291751  0.589252357  0.68893515\n",
              "abruptly        -0.505668540  0.335404427  0.097786019 -0.388914190 -0.38557285\n",
              "absorbing       -0.287205026 -0.312171091 -0.357957836  0.167526479  0.42167415\n",
              "accomplishments  0.296464644  0.184565544  0.242916970 -0.297316351 -0.02192918\n",
              "accused          0.603896149  0.979517578 -0.196961453 -0.328864534 -0.58027338\n",
              "achievement     -0.005924459  0.660246560 -0.723082428 -0.149632690 -0.13746019\n",
              "acquired        -0.439290708  0.152316245 -0.474707575 -0.062887407  0.03274934\n",
              "acres           -0.256821371  0.664099741  0.234280279  0.411153731  0.39836951\n",
              "adams            0.366176218 -0.230617141 -0.443790956 -0.056869235  0.19434052\n",
              "address         -0.299842167  0.005570082 -0.697858051  0.967882308  0.74267883\n",
              "adjust          -0.161753172 -0.110804855 -0.080501837 -0.833086287  0.18999399\n",
              "admires          0.750364485 -0.452595970 -0.414619174 -0.129256578 -0.12132017\n",
              "admits          -0.496023417 -0.126543225 -0.613394837  0.450257382  0.47289568\n",
              "admitted        -0.304267374  0.291767974 -0.205611595  0.173763380 -0.42971896\n",
              "adorn            0.737974531 -0.031574930 -0.756253249  0.190669169 -0.43936459\n",
              "adrenalin       -0.237778428  0.292461247 -0.500908995 -0.029012521  0.30806042\n",
              "adress          -0.269471441  0.175601141 -0.146203963  0.245779959 -0.07227204\n",
              "adrift           0.555799185  0.612421355 -0.834533678 -0.337302176  0.22362290\n",
              "advanced        -0.361123811 -0.306083825  0.121554167  0.802122076  0.53786073\n",
              "⋮               ⋮            ⋮            ⋮            ⋮            ⋮          \n",
              "your             1.10687310  -0.02433232  1.1524797    -0.511380807 -0.31819395\n",
              "she             -0.09693033  -0.34142356  1.4555342    -0.502895692  0.11742506\n",
              "love             1.10917708  -0.83019695  1.6898825     0.526807611 -0.03894946\n",
              "they             0.65779104  -1.27628312  1.0787799    -0.648060347 -0.79852493\n",
              "all              1.30020891  -0.08803566  0.8815118    -0.885388090 -0.67187065\n",
              "as               0.12374327  -0.20070733  1.3998739    -0.623881007 -0.67044334\n",
              "for              0.76098587   0.20402389  1.0796115    -0.136541600 -0.84632118\n",
              "be               0.87071768  -0.36572406  1.5257384     0.333888251 -0.39944828\n",
              "when             0.53755694  -0.34664721  1.2436099    -0.448986056 -0.60906069\n",
              "with             1.00956877  -0.65822579  0.6530590    -1.009942032 -0.70892681\n",
              "so               0.66041415  -0.25366041  1.9301640    -0.401711816 -0.60208501\n",
              "on               0.68217380  -0.01680861  0.7734562    -1.472068964 -0.97036547\n",
              "was              0.33248939   0.01220992  1.2473095    -0.494631285 -0.82375822\n",
              "but              0.55861198  -0.22122495  1.4829625     0.004946889 -0.48550488\n",
              "he              -0.25155399  -0.22799796  1.4709812    -0.989057028 -0.09478956\n",
              "like             0.56541311  -0.13815864  1.4804062    -0.317186583 -0.70301863\n",
              "that             0.43860085  -0.51347366  1.6675639    -0.123437992 -0.38485505\n",
              "are              2.17995070  -0.43209494  0.7141540    -0.310214277 -0.62570673\n",
              "me               0.82089915  -1.02258828  1.7741451     0.354590480  0.63372518\n",
              "of               1.11015049   0.70160431  0.6867089    -0.244857004 -0.37555101\n",
              "in               0.67796794   0.43627938  0.7676684    -1.628720396 -0.29710615\n",
              "it               0.40555803   0.26815605  1.6548084    -0.330136184 -0.38485989\n",
              "you              0.56709298  -1.09163977  1.8576337     0.325543294 -0.27147131\n",
              "my               0.93463658  -0.11169862  1.5346297    -0.788939348 -0.27372689\n",
              "is               0.78262696   1.31974635  1.8564849     0.246831066 -0.62006157\n",
              "to               0.31924556  -0.67267930  1.7620503    -0.505294399 -0.34906607\n",
              "and              0.81325745  -0.25105040  0.9476438    -0.762718670 -0.34686658\n",
              "a               -0.91003319  -0.21015546  1.3258001     0.041061325 -1.26043371\n",
              "the              0.13211323   0.45814443  0.9946228    -1.611777582  0.13637256\n",
              "i                0.44235429  -0.87338609  1.5814970    -0.295314993 -0.63478018\n",
              "                [,20]        [,21]      \n",
              "1837             0.25301617   0.22173371\n",
              "1841            -0.25701126  -0.47245659\n",
              "1881            -0.20560002   0.68405675\n",
              "2005            -0.45100058   0.20574856\n",
              "36              -0.16733290  -0.24080546\n",
              "38              -0.21039644   0.01838548\n",
              "39              -0.26660598  -0.24322074\n",
              "52               0.46920537   0.53592542\n",
              "5â              -0.99653031  -0.42381830\n",
              "600              0.31026247   0.24424602\n",
              "abcdefg          0.17785692  -0.61088021\n",
              "abroad          -0.29230629   0.60544112\n",
              "abruptly         0.10788126   1.07054095\n",
              "absorbing       -0.15069332   0.27986481\n",
              "accomplishments  0.02381857   1.01208497\n",
              "accused         -0.52653290   0.45906009\n",
              "achievement     -0.06720847   0.22176334\n",
              "acquired        -0.24725423   0.24409540\n",
              "acres            0.05258725  -0.32525395\n",
              "adams            0.12382523   0.04268834\n",
              "address         -0.23478951  -0.08638539\n",
              "adjust          -0.60048538   0.52134227\n",
              "admires         -0.75372962   0.85117923\n",
              "admits           0.25420994  -0.19119665\n",
              "admitted        -0.54863718   0.64700551\n",
              "adorn           -0.23082647   0.11158748\n",
              "adrenalin        0.25517963   0.66449649\n",
              "adress          -0.52550125   0.23499252\n",
              "adrift          -0.09379581   0.11036846\n",
              "advanced        -0.87075226  -0.35036325\n",
              "⋮               ⋮            ⋮          \n",
              "your             0.021489723  0.26303308\n",
              "she             -0.016573978  0.85044696\n",
              "love             0.456800124 -0.89155241\n",
              "they             0.551091693 -0.14490979\n",
              "all              0.524857500 -0.40063246\n",
              "as               0.337694542  0.11456077\n",
              "for             -0.184942038 -0.51611075\n",
              "be               0.806497218  0.14134930\n",
              "when             0.226135761  0.11392773\n",
              "with             0.265583915 -0.69986409\n",
              "so               0.260621312  0.06687380\n",
              "on               0.680163351 -0.55308657\n",
              "was              0.323448808  0.44993179\n",
              "but              0.359658026  0.21088970\n",
              "he               0.002057899  1.02377057\n",
              "like             0.960619367 -0.35390149\n",
              "that             0.190445567  0.02113333\n",
              "are              0.790375902 -0.38930963\n",
              "me               0.275596054  0.36610939\n",
              "of              -0.183021007 -1.45380483\n",
              "in               0.173605579 -1.31176620\n",
              "it               0.355766796 -0.56673340\n",
              "you              0.545372987 -0.13519577\n",
              "my              -0.436236316  0.39523997\n",
              "is               0.448739600 -0.52222979\n",
              "to               0.126604838 -0.54660212\n",
              "and              0.076044470 -0.30018538\n",
              "a                0.094672523 -0.58245419\n",
              "the              0.509871545 -1.10640435\n",
              "i                0.355809566 -0.27658641"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoS90zpVNs4O"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arghyAVINGtH"
      },
      "source": [
        "Now we can begin to play. Similarly to standard correlation, we can look at comparing two vectors using **cosine similarity**. Let's see what is similar with 'school':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "XLSKt6-vN2_i",
        "outputId": "965ef907-5bf1-42fc-f8a0-96fb62f5e9d4"
      },
      "source": [
        "# Word vector for school\n",
        "school <- word_vectors[\"school\", , drop = FALSE]\n",
        "\n",
        "# Cosine similarity\n",
        "school_cos_sim <- sim2(x = word_vectors, y = school, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top ten words relating to school\n",
        "head(sort(school_cos_sim[,1], decreasing = TRUE), 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>school</dt><dd>1</dd><dt>work</dt><dd>0.723260024039281</dd><dt>pool</dt><dd>0.707045282579403</dd><dt>time</dt><dd>0.698334963531082</dd><dt>today</dt><dd>0.694419937213708</dd><dt>go</dt><dd>0.670469123934321</dd><dt>fun</dt><dd>0.65918242652469</dd><dt>home</dt><dd>0.658261509303435</dd><dt>day</dt><dd>0.64708924923481</dd><dt>cool</dt><dd>0.63896152793748</dd></dl>\n"
            ],
            "text/markdown": "school\n:   1work\n:   0.723260024039281pool\n:   0.707045282579403time\n:   0.698334963531082today\n:   0.694419937213708go\n:   0.670469123934321fun\n:   0.65918242652469home\n:   0.658261509303435day\n:   0.64708924923481cool\n:   0.63896152793748\n\n",
            "text/latex": "\\begin{description*}\n\\item[school] 1\n\\item[work] 0.723260024039281\n\\item[pool] 0.707045282579403\n\\item[time] 0.698334963531082\n\\item[today] 0.694419937213708\n\\item[go] 0.670469123934321\n\\item[fun] 0.65918242652469\n\\item[home] 0.658261509303435\n\\item[day] 0.64708924923481\n\\item[cool] 0.63896152793748\n\\end{description*}\n",
            "text/plain": [
              "   school      work      pool      time     today        go       fun      home \n",
              "1.0000000 0.7232600 0.7070453 0.6983350 0.6944199 0.6704691 0.6591824 0.6582615 \n",
              "      day      cool \n",
              "0.6470892 0.6389615 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCiI8fafOUJR"
      },
      "source": [
        "Obviously, school is the most similar to school. Based on the poems that the children wrote, we can also see words like 'work', 'fun' and 'home' as most similar to 'school'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9NuW-5GzNAo"
      },
      "source": [
        "## Pet example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FQVER95zFn6"
      },
      "source": [
        "Let's try our pet example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZlyQDS9EzYpY",
        "outputId": "09246cc0-4154-4685-c926-2a627a49bfa1"
      },
      "source": [
        "# cat - meow + bark should equal dog\n",
        "dog <- word_vectors[\"cat\", , drop = FALSE] -\n",
        "  word_vectors[\"meow\", , drop = FALSE] +\n",
        "  word_vectors[\"bark\", , drop = FALSE]\n",
        "  \n",
        "# Calculates pairwise similarities between the rows of two matrices\n",
        "dog_cos_sim <- sim2(x = word_vectors, y = dog, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(sort(dog_cos_sim[,1], decreasing = TRUE), 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>dog</dt><dd>0.826055483525039</dd><dt>cat</dt><dd>0.769928431206193</dd><dt>he</dt><dd>0.660280417305649</dd><dt>bark</dt><dd>0.651414513421272</dd><dt>fat</dt><dd>0.646069932616433</dd></dl>\n"
            ],
            "text/markdown": "dog\n:   0.826055483525039cat\n:   0.769928431206193he\n:   0.660280417305649bark\n:   0.651414513421272fat\n:   0.646069932616433\n\n",
            "text/latex": "\\begin{description*}\n\\item[dog] 0.826055483525039\n\\item[cat] 0.769928431206193\n\\item[he] 0.660280417305649\n\\item[bark] 0.651414513421272\n\\item[fat] 0.646069932616433\n\\end{description*}\n",
            "text/plain": [
              "      dog       cat        he      bark       fat \n",
              "0.8260555 0.7699284 0.6602804 0.6514145 0.6460699 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZNvOiUXynMr"
      },
      "source": [
        "Success - our predicted result was correct! We get 'dog' as the highest predicted result after the one we used (cat). We can think of this scenario as cats say meow and dogs say bark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO8mYP8fzIPY"
      },
      "source": [
        "## Parent example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_sKk9O-z4Tf"
      },
      "source": [
        "Let's move on to the parent example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "tk1jjEFlwDKB",
        "outputId": "dd81b86b-a209-4b4b-c1bd-3e14a4ccbe7b"
      },
      "source": [
        "# mom - girl + boy should equal dad\n",
        "dad <- word_vectors[\"mom\", , drop = FALSE] -\n",
        "  word_vectors[\"girl\", , drop = FALSE] +\n",
        "  word_vectors[\"boy\", , drop = FALSE]\n",
        "  \n",
        "# Calculates pairwise similarities between the rows of two matrices\n",
        "dad_cos_sim <- sim2(x = word_vectors, y = dad, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(sort(dad_cos_sim[,1], decreasing = TRUE), 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>mom</dt><dd>0.875838592145447</dd><dt>dad</dt><dd>0.835486220713697</dd><dt>brother</dt><dd>0.73273162006622</dd><dt>said</dt><dd>0.668738320492883</dd><dt>says</dt><dd>0.644734826267501</dd></dl>\n"
            ],
            "text/markdown": "mom\n:   0.875838592145447dad\n:   0.835486220713697brother\n:   0.73273162006622said\n:   0.668738320492883says\n:   0.644734826267501\n\n",
            "text/latex": "\\begin{description*}\n\\item[mom] 0.875838592145447\n\\item[dad] 0.835486220713697\n\\item[brother] 0.73273162006622\n\\item[said] 0.668738320492883\n\\item[says] 0.644734826267501\n\\end{description*}\n",
            "text/plain": [
              "      mom       dad   brother      said      says \n",
              "0.8758386 0.8354862 0.7327316 0.6687383 0.6447348 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F5XmJd60Ayg"
      },
      "source": [
        "'Dad' was a top result. Finally, let's try the infamous king and queen example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS-z1CCx8yOy"
      },
      "source": [
        "## King and queen example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "YFGQfG7i8xSC",
        "outputId": "03cdb72e-bb08-4259-8bf3-118392da9f85"
      },
      "source": [
        "# king - man + woman should equal queen\n",
        "queen <- word_vectors[\"king\", , drop = FALSE] -\n",
        "  word_vectors[\"man\", , drop = FALSE] +\n",
        "  word_vectors[\"woman\", , drop = FALSE]\n",
        "\n",
        "# Calculate pairwise similarities\n",
        "queen_cos_sim = sim2(x = word_vectors, y = queen, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(sort(queen_cos_sim[,1], decreasing = TRUE), 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>king</dt><dd>0.747590200931503</dd><dt>martin</dt><dd>0.667430956938263</dd><dt>kong</dt><dd>0.624612945436619</dd><dt>queen</dt><dd>0.58490231995137</dd><dt>luther</dt><dd>0.581086583135822</dd></dl>\n"
            ],
            "text/markdown": "king\n:   0.747590200931503martin\n:   0.667430956938263kong\n:   0.624612945436619queen\n:   0.58490231995137luther\n:   0.581086583135822\n\n",
            "text/latex": "\\begin{description*}\n\\item[king] 0.747590200931503\n\\item[martin] 0.667430956938263\n\\item[kong] 0.624612945436619\n\\item[queen] 0.58490231995137\n\\item[luther] 0.581086583135822\n\\end{description*}\n",
            "text/plain": [
              "     king    martin      kong     queen    luther \n",
              "0.7475902 0.6674310 0.6246129 0.5849023 0.5810866 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rOg8QQY_Sbq"
      },
      "source": [
        "Unfortunately, queen came in at 4th. Let's try changing **man** and **woman** to **boy** and **girl** to account for the kid's writting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "zu15bTVZ_gp_",
        "outputId": "b1f5fb2b-168a-4626-a243-5bb28916704d"
      },
      "source": [
        "# king - boy + girl should equal queen\n",
        "queen <- word_vectors[\"king\", , drop = FALSE] -\n",
        "  word_vectors[\"boy\", , drop = FALSE] +\n",
        "  word_vectors[\"girl\", , drop = FALSE]\n",
        "\n",
        "# Calculate pairwise similarities\n",
        "queen_cos_sim = sim2(x = word_vectors, y = queen, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(sort(queen_cos_sim[,1], decreasing = TRUE), 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>king</dt><dd>0.798323772159053</dd><dt>queen</dt><dd>0.659375241411339</dd><dt>bully</dt><dd>0.549611924435534</dd><dt>owner</dt><dd>0.483410830291539</dd><dt>african</dt><dd>0.474792438434143</dd></dl>\n"
            ],
            "text/markdown": "king\n:   0.798323772159053queen\n:   0.659375241411339bully\n:   0.549611924435534owner\n:   0.483410830291539african\n:   0.474792438434143\n\n",
            "text/latex": "\\begin{description*}\n\\item[king] 0.798323772159053\n\\item[queen] 0.659375241411339\n\\item[bully] 0.549611924435534\n\\item[owner] 0.483410830291539\n\\item[african] 0.474792438434143\n\\end{description*}\n",
            "text/plain": [
              "     king     queen     bully     owner   african \n",
              "0.7983238 0.6593752 0.5496119 0.4834108 0.4747924 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeEJQciGAcdm"
      },
      "source": [
        "It worked!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Tangent on Bias\n",
        "\n",
        "As we discussed in class, word embeddings have proven to be a useful tool for uncovering/revealing bias in large corpora. Here, we can see how well the kids fare. We'll look at occupations. "
      ],
      "metadata": {
        "id": "mvXWRuybd8bh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "6EHjaWj1Q6ie",
        "outputId": "abe54fec-04c1-4f5b-a99a-ac9f2f263faf"
      },
      "source": [
        "job <- word_vectors[\"job\", , drop = FALSE] -\n",
        "  word_vectors[\"boy\", , drop = FALSE] +\n",
        "  word_vectors[\"girl\", , drop = FALSE]\n",
        "\n",
        "# Calculate pairwise similarities\n",
        "job_cos_sim = sim2(x = word_vectors, y = job, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(sort(job_cos_sim[,1], decreasing = TRUE), 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>job</dt><dd>0.715648824942142</dd><dt>class</dt><dd>0.56218893875595</dd><dt>done</dt><dd>0.534019315473513</dd><dt>math</dt><dd>0.510539490938901</dd><dt>teacher</dt><dd>0.495841776065954</dd></dl>\n"
            ],
            "text/markdown": "job\n:   0.715648824942142class\n:   0.56218893875595done\n:   0.534019315473513math\n:   0.510539490938901teacher\n:   0.495841776065954\n\n",
            "text/latex": "\\begin{description*}\n\\item[job] 0.715648824942142\n\\item[class] 0.56218893875595\n\\item[done] 0.534019315473513\n\\item[math] 0.510539490938901\n\\item[teacher] 0.495841776065954\n\\end{description*}\n",
            "text/plain": [
              "      job     class      done      math   teacher \n",
              "0.7156488 0.5621889 0.5340193 0.5105395 0.4958418 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "AT6TXy8sRDvL",
        "outputId": "4ad22dd4-be83-48bb-a7a4-d3c685f804ff"
      },
      "source": [
        "job <- word_vectors[\"job\", , drop = FALSE] -\n",
        "  word_vectors[\"girl\", , drop = FALSE] +\n",
        "  word_vectors[\"boy\", , drop = FALSE]\n",
        "\n",
        "# Calculate pairwise similarities\n",
        "job_cos_sim = sim2(x = word_vectors, y = job, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(sort(job_cos_sim[,1], decreasing = TRUE), 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>job</dt><dd>0.842859293138359</dd><dt>dinner</dt><dd>0.599269729363558</dd><dt>nick</dt><dd>0.497523991301448</dd><dt>ted</dt><dd>0.495244063849114</dd><dt>bob</dt><dd>0.486969764215646</dd></dl>\n"
            ],
            "text/markdown": "job\n:   0.842859293138359dinner\n:   0.599269729363558nick\n:   0.497523991301448ted\n:   0.495244063849114bob\n:   0.486969764215646\n\n",
            "text/latex": "\\begin{description*}\n\\item[job] 0.842859293138359\n\\item[dinner] 0.599269729363558\n\\item[nick] 0.497523991301448\n\\item[ted] 0.495244063849114\n\\item[bob] 0.486969764215646\n\\end{description*}\n",
            "text/plain": [
              "      job    dinner      nick       ted       bob \n",
              "0.8428593 0.5992697 0.4975240 0.4952441 0.4869698 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting! We're not seeing the same dynamics observed in other settings. Given the small corpus, though, we'd want to add a lot more data before we could be confident that those biases weren't present here."
      ],
      "metadata": {
        "id": "YGaGPvmRAqSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Estimated Embeddings"
      ],
      "metadata": {
        "id": "tsKGecGLCP9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With that in hand, we can estimate a simple clustering algorithm. We specify 5 clusters, but feel free to play around with that number."
      ],
      "metadata": {
        "id": "YlF5IxbVGitT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set.seed(12345)\n",
        "clusters <- kmeans(word_vectors, centers = 5, iter.max  = 30)"
      ],
      "metadata": {
        "id": "q0ZO9qPsGnno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note what we have estimated with KMeans. We have 5 cluster centers, each of 50 dimensions, the same number of dimensions that we have for each of our tokens. Therefore, we look for which of the tokens are most similar to one of our cluster centers. "
      ],
      "metadata": {
        "id": "XvDjFDOmGoEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster1 <- t(as.matrix(clusters$centers[1,]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "qzYg8qWtIHWO",
        "outputId": "64d91b89-758b-48c7-9985-e2646403a045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>1</li><li>50</li></ol>\n"
            ],
            "text/markdown": "1. 1\n2. 50\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 1\n\\item 50\n\\end{enumerate*}\n",
            "text/plain": [
              "[1]  1 50"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>1</li><li>50</li></ol>\n"
            ],
            "text/markdown": "1. 1\n2. 50\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 1\n\\item 50\n\\end{enumerate*}\n",
            "text/plain": [
              "[1]  1 50"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clus_cos_sim = sim2(x = word_vectors, y = cluster1, method = \"cosine\", norm = \"l2\")\n"
      ],
      "metadata": {
        "id": "lX4qDDhYISjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top ten cluster words\n",
        "head(sort(clus_cos_sim[,1], decreasing = TRUE), 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "aGxB3wakJYen",
        "outputId": "1486d94f-1488-4235-8a69-3a9a324e8862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>moonbeam</dt><dd>0.729259707365738</dd><dt>mutant</dt><dd>0.712216005328542</dd><dt>hanky</dt><dd>0.66036923022911</dd><dt>mercedes</dt><dd>0.655440779983402</dd><dt>hawaiian</dt><dd>0.63764664274969</dd><dt>louse</dt><dd>0.637353558105925</dd><dt>thers</dt><dd>0.632174447354251</dd><dt>weres</dt><dd>0.630761448297422</dd><dt>roach</dt><dd>0.626877647151922</dd><dt>lama</dt><dd>0.624460076424331</dd></dl>\n"
            ],
            "text/markdown": "moonbeam\n:   0.729259707365738mutant\n:   0.712216005328542hanky\n:   0.66036923022911mercedes\n:   0.655440779983402hawaiian\n:   0.63764664274969louse\n:   0.637353558105925thers\n:   0.632174447354251weres\n:   0.630761448297422roach\n:   0.626877647151922lama\n:   0.624460076424331\n\n",
            "text/latex": "\\begin{description*}\n\\item[moonbeam] 0.729259707365738\n\\item[mutant] 0.712216005328542\n\\item[hanky] 0.66036923022911\n\\item[mercedes] 0.655440779983402\n\\item[hawaiian] 0.63764664274969\n\\item[louse] 0.637353558105925\n\\item[thers] 0.632174447354251\n\\item[weres] 0.630761448297422\n\\item[roach] 0.626877647151922\n\\item[lama] 0.624460076424331\n\\end{description*}\n",
            "text/plain": [
              " moonbeam    mutant     hanky  mercedes  hawaiian     louse     thers     weres \n",
              "0.7292597 0.7122160 0.6603692 0.6554408 0.6376466 0.6373536 0.6321744 0.6307614 \n",
              "    roach      lama \n",
              "0.6268776 0.6244601 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's loop over the cluster centers:"
      ],
      "metadata": {
        "id": "umhocUbFJmmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topWordMatrix <- matrix(NA, 5,10)\n",
        "\n",
        "for (i in 1:5){\n",
        "  cluster <- t(as.matrix(clusters$centers[i,]))\n",
        "  clus_cos_sim = sim2(x = word_vectors, y = cluster, method = \"cosine\", norm = \"l2\")\n",
        "  topWordMatrix[i,] <- names(head(sort(clus_cos_sim[,1], decreasing = TRUE), 10))\n",
        "}"
      ],
      "metadata": {
        "id": "xw9rj-2zJuWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topWordMatrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "1oluWhm3KX77",
        "outputId": "76351209-243d-413e-e42f-64d6799c682d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 5 × 10 of type chr</caption>\n",
              "<tbody>\n",
              "\t<tr><td>moonbeam</td><td>mutant   </td><td>hanky  </td><td>mercedes</td><td>hawaiian</td><td>louse </td><td>thers    </td><td>weres  </td><td>roach    </td><td>lama       </td></tr>\n",
              "\t<tr><td>dont't  </td><td>literally</td><td>fickle </td><td>dating  </td><td>sag     </td><td>marty </td><td>struggled</td><td>tempted</td><td>carlos   </td><td>reluctantly</td></tr>\n",
              "\t<tr><td>defiance</td><td>mah      </td><td>loveâ’s</td><td>settling</td><td>chilli  </td><td>blue's</td><td>strides  </td><td>plops  </td><td>swooshing</td><td>yang       </td></tr>\n",
              "\t<tr><td>but     </td><td>just     </td><td>because</td><td>that    </td><td>when    </td><td>you   </td><td>if       </td><td>now    </td><td>not      </td><td>all        </td></tr>\n",
              "\t<tr><td>surface </td><td>mist     </td><td>bag    </td><td>filling </td><td>atop    </td><td>pile  </td><td>pot      </td><td>freezer</td><td>fills    </td><td>gate       </td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA matrix: 5 × 10 of type chr\n\n| moonbeam | mutant    | hanky   | mercedes | hawaiian | louse  | thers     | weres   | roach     | lama        |\n| dont't   | literally | fickle  | dating   | sag      | marty  | struggled | tempted | carlos    | reluctantly |\n| defiance | mah       | loveâ’s | settling | chilli   | blue's | strides   | plops   | swooshing | yang        |\n| but      | just      | because | that     | when     | you    | if        | now     | not       | all         |\n| surface  | mist      | bag     | filling  | atop     | pile   | pot       | freezer | fills     | gate        |\n\n",
            "text/latex": "A matrix: 5 × 10 of type chr\n\\begin{tabular}{llllllllll}\n\t moonbeam & mutant    & hanky   & mercedes & hawaiian & louse  & thers     & weres   & roach     & lama       \\\\\n\t dont't   & literally & fickle  & dating   & sag      & marty  & struggled & tempted & carlos    & reluctantly\\\\\n\t defiance & mah       & loveâ’s & settling & chilli   & blue's & strides   & plops   & swooshing & yang       \\\\\n\t but      & just      & because & that     & when     & you    & if        & now     & not       & all        \\\\\n\t surface  & mist      & bag     & filling  & atop     & pile   & pot       & freezer & fills     & gate       \\\\\n\\end{tabular}\n",
            "text/plain": [
              "     [,1]     [,2]      [,3]    [,4]     [,5]     [,6]   [,7]      [,8]   \n",
              "[1,] moonbeam mutant    hanky   mercedes hawaiian louse  thers     weres  \n",
              "[2,] dont't   literally fickle  dating   sag      marty  struggled tempted\n",
              "[3,] defiance mah       loveâ’s settling chilli   blue's strides   plops  \n",
              "[4,] but      just      because that     when     you    if        now    \n",
              "[5,] surface  mist      bag     filling  atop     pile   pot       freezer\n",
              "     [,9]      [,10]      \n",
              "[1,] roach     lama       \n",
              "[2,] carlos    reluctantly\n",
              "[3,] swooshing yang       \n",
              "[4,] not       all        \n",
              "[5,] fills     gate       "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A little hard to see too much coming through in the way of the clusters here. Play around with the specifications to see if shifting the number of clusters in KMeans, the size of the window in GloVe, etc. get us to more sensible clusters. If not, it may just be that the dataset is too limited to really learn much."
      ],
      "metadata": {
        "id": "AucbV4LGKu9Y"
      }
    }
  ]
}